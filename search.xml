<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>强化学习笔记（四）-蒙特卡洛方法</title>
      <link href="/2023/10/10/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/"/>
      <url>/2023/10/10/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="强化学习笔记（四）—蒙特卡洛方法"><a href="#强化学习笔记（四）—蒙特卡洛方法" class="headerlink" title="强化学习笔记（四）—蒙特卡洛方法"></a>强化学习笔记（四）—蒙特卡洛方法</h1><h2 id="预置知识"><a href="#预置知识" class="headerlink" title="预置知识"></a>预置知识</h2><p>模型：可以理解为强化学习中的环境。例如，$ p(r|s,a) $以及$p(s’|s,a)$ 都是取决于环境的概率。我们用概率来表示与环境交互后环境给出的结果，可以理解为我们对环境进行了建模。model-based强化学习，一般是说，我们首先对环境有一个定量的认识，可能会用数据估计出一个模型，然后基于对环境的建模进行训练；而model-free强化学习，可以认为是在对环境一无所知的情况下训练agent。</p><p>蒙特卡洛方法是一种model-free的方法。蒙特卡洛方法也被称为统计模拟方法，是指通过使用随机数来解决很多计算问题的方法。他的工作原理就是两件事：不断抽样、逐渐逼近。</p><p>在这里，我们需要温习一下概率论中的大数定理：</p><p><img src="https://zlisnail.cn/img/image-20230918203422649.png" alt="image-20230918203422649"></p><p>这个定理说明，当我们采集了足够多的样本时，样本的平均数可以代表随机变量的期望值。按照赵老师的话说，就是monte-carlo estimation可以用来求解mean estimation的问题。 </p><h2 id="强化学习中的Monte-Carlo算法"><a href="#强化学习中的Monte-Carlo算法" class="headerlink" title="强化学习中的Monte-Carlo算法"></a>强化学习中的Monte-Carlo算法</h2><h3 id="MC-Basic-algorithm"><a href="#MC-Basic-algorithm" class="headerlink" title="MC Basic algorithm"></a>MC Basic algorithm</h3><p>该算法的关键在于，将policy iteration algorithm转化为model-free的。</p><p>复习一下策略迭代算法。它分为两个迭代步骤：PE与PI。</p><p><img src="https://zlisnail.cn/img/image-20230918211358871.png" alt="image-20230918211358871"></p><p>我们考虑PI。在PI中，$q_{π_k}(s,a)$是依赖于模型的。但是，我们可以从它的定义出发，将它写成另一种不依赖于模型的形式：</p><p><img src="https://zlisnail.cn/img/image-20230918211946086.png" alt="image-20230918211946086"></p><p>正如上图所说，我们可以基于与环境的交互数据（experience），求解这个mean estimation问题。具体步骤如下：</p><p><img src="https://zlisnail.cn/img/image-20230918215952791.png" alt=""></p><p>于是，就可以将policy iteration algorithm进行改进。PI不需要改动，只需要PE步骤替换为上图的算法：</p><p><img src="https://zlisnail.cn/img/image-20230918220627254.png" alt="image-20230918220627254"></p><p>也就是说，评估策略的方法，从原先计算state-value，到现在计算每个步骤的q值。</p><p>伪代码如下：</p><p><img src="https://zlisnail.cn/img/image-20230918221443535.png" alt="image-20230918221443535"></p><p>这就是MC Basic算法，这是赵老师自己起的名字，所以在其他教材中可能看不到这个名词。这个算法因为太过低效，所以没有实践价值，但是有助于理解MC方法。由于policy iteration algorithm本身是收敛的，所以MC Basic也是收敛的。</p><p>这里每次计算q值的一次episode的长度，同样也是适当即可。太长的话对于改善策略没有实际的帮助，而太短则会导致策略得不到最优策略。</p><h3 id="MC-exploring-starts"><a href="#MC-exploring-starts" class="headerlink" title="MC exploring starts"></a>MC exploring starts</h3><p>由于MC basic方法效率太低，现在我们来探索一种方法，来更高效地利用数据、更新策略。</p><h4 id="如何改进MC-basic"><a href="#如何改进MC-basic" class="headerlink" title="如何改进MC basic"></a>如何改进MC basic</h4><p>在MC basic当中，每次对于一个（s,a）的episode可以用下面的trajectory来表示：<br><img src="https://zlisnail.cn/img/image-20231009153413242.png" alt="image-20231009153413242"></p><p>其中，红圈是一个state-action pair。我们定义在一个episode中，state-action pair 每出现一次，就是一次visit。</p><p>对于一个episode，可以改进它的利用方法。在MC basic方法中，我们仅仅利用了第一个state-action pair的q值（这种方法称作<strong>initial-visit method</strong>）。实际上，episode中还有很多state-action pair，以它们本身为界，之后的trajectory也可以看做是它们的一次q值采样。如下图所示：<br><img src="https://zlisnail.cn/img/image-20231009154324033.png" alt="image-20231009154324033"></p><p>这种方法叫做<strong>data-efficient method</strong>。这种方法又分为两种处理方法：<br>一种是<strong>first-visit method</strong>。在一个episode中，同一个pair可能多次出现。first-visit method只会利用第一次出现的q值做迭代，也即使用trajectory最长的一次。</p><p>另一种是<strong>every-visit method</strong>。多次出现的pair，出现的不同q值，都会被利用。</p><hr><p>上面，我们讨论的是如何更高效地利用episode得到的数据。下面，我们讨论如何更快速地更新策略。</p><p>在MC basic中，我们在PE步骤先收集一个pair的所有episode结果，然后对他们求平均来估计q值。这种方法必须等待多个episode的结果，才能最终得到这次迭代的q值估计。</p><p>现在，我们每得到一次episode的结果，我们就用它来估计q值，进而PI改进策略。improve the policy episode-by-episode。显然，精确度会下降，但它不影响最终可以收敛到一个策略当中。</p><blockquote><p>GPI（generalized policy iteration），指的是一种算法框架。它所代表的的思想是，在PE与PI之间不断地切换，且PE得到的估计的v值或者q值不需要特别地精确。在PE与PI交替进行足够多次之后，一定能够收敛到一个最优策略中去。许多强化学习算法最终都会落入这个框架当中。</p><p><img src="https://zlisnail.cn/img/image-20231009163105135.png" alt="image-20231009163105135"></p></blockquote><h4 id="具体算法"><a href="#具体算法" class="headerlink" title="具体算法"></a>具体算法</h4><p>​    伪代码如下：</p><p><img src="https://zlisnail.cn/img/image-20231009164257996.png" alt="image-20231009164257996"></p><p>算法中可以发现一些应用了上述改进思想的点：</p><ul><li>这里是每次做一次episode，之后就立刻进行PI，改进策略，策略更新更加高效。该算法之比于MC basic，其实就类似于policy iteration之比于value iteration。</li><li>算法应用的是first-visit method，也可以改成every-visit method。</li><li>倒序考虑每个pair的g值，算法更高效。</li><li>做一次episode的始态pair是有讲究的。一个pair，要么出现在episode中间（visit），要么是一个episode的始态（start）。为了算法最终能够对所有pair都进行估计（防止我们遗漏掉一些pair，而这个pair恰巧在最优策略当中），最终，我们的算法要确保每个pair都至少作为一次episode的始态。这就是<strong>exploring starts</strong> 名字的由来。也因此，这种算法很难应用于实践。</li></ul><h3 id="MC-without-exploring-starts-MC-Epsilon-Greedy"><a href="#MC-without-exploring-starts-MC-Epsilon-Greedy" class="headerlink" title="MC without exploring starts-MC Epsilon-Greedy"></a>MC without exploring starts-MC Epsilon-Greedy</h3><p>由于exploring starts这样的性质过于耗时，现在，我们要想办法把它去掉。去掉的方式，是引入<strong>soft policy</strong>。所谓的soft，指的是策略有可能会选择任何一个动作，对于任何一个动作的概率均不为零。</p><blockquote><p>policy 分为deterministic policy （<em>e.g.</em> greedy policy）和 stochastic policy（<em>e.g.</em> soft policy）。</p></blockquote><p>对于soft policy，一个足够长的episode可以遍历所有的pair，从而可以去掉exploring starts的条件。</p><hr><p>现在，我们将soft policy中的ε-greedy policy融合在MC exploring starts方法当中去。ε-greedy policy定义如下：</p><p> <img src="https://zlisnail.cn/img/image-20231009172217514.png" alt="image-20231009172217514"></p><p>公式看起来有些复杂，实际上，ε是<strong>探索（exploration）</strong>的概率，而1-ε是<strong>利用（exploitation）</strong>的概率。利用时，就采取最优动作；探索时，就随机采取动作。ε的值决定了探索与利用的比例。  </p><p>要将它融合进算法中，非常简单，只需要选出最大q值的动作放在exploitation公式里即可：</p><p><img src="https://zlisnail.cn/img/image-20231010170815448.png" alt="image-20231010170815448"></p><p>伪代码如下：</p><p><img src="https://zlisnail.cn/img/image-20231009230741781.png" alt="image-20231009230741781"></p><p>这里使用了every-visit，这是因为episode少了，但是变长了，如果只用第一个g值会造成浪费。</p><blockquote><p>伪代码中，虽然写明了是用的every-visit，但是实际算法实现的似乎还是first-visit？待确认</p></blockquote><h4 id="实际效果"><a href="#实际效果" class="headerlink" title="实际效果"></a>实际效果</h4><p>在经典grid-world环境中，每次生成1000,000步长的episode，有机会两次迭代就能获得最优ε-greedy策略。</p><p><img src="https://zlisnail.cn/img/image-20231009233428911.png" alt="image-20231009233428911"></p><p>ε-greedy方法去除了exploring starts后，牺牲了最优性。我们只能通过它找到最优的ε-greedy策略。如下面的例子：</p><p><img src="https://zlisnail.cn/img/image-20231009235956900.png" alt="image-20231009235956900"></p><p>上图中，ε=0和ε=0.1时，它们的最优策略是<strong>一致的(consistence)</strong>，即每个状态上的最优动作是相同的。但是，当eps=0.2或0.5时，一致性就丢失了。在强化学习训练成熟后，实际应用中肯定不能用ε-greedy最优策略，因此，保持ε-greedy策略与greedy策略的一致性是很重要的，这就要求ε值不能太大。</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> 强化学习数学原理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>猫变虎</title>
      <link href="/2023/10/02/%E7%8C%AB%E5%8F%98%E8%99%8E/"/>
      <url>/2023/10/02/%E7%8C%AB%E5%8F%98%E8%99%8E/</url>
      
        <content type="html"><![CDATA[<h1 id="猫变虎"><a href="#猫变虎" class="headerlink" title="猫变虎"></a>猫变虎</h1><h2 id="项目目标"><a href="#项目目标" class="headerlink" title="项目目标"></a>项目目标</h2><p>现有两张图片，一只小猫和一只老虎：</p><p><img src="https://zlisnail.cn/img/kitty.png" alt="kitty"></p><p><img src="https://zlisnail.cn/img/tiger.png" alt="tiger"></p><p>要求将上图变为下图，并展示中间的渐变过程。</p><h2 id="代码运行声明"><a href="#代码运行声明" class="headerlink" title="代码运行声明"></a>代码运行声明</h2><p>项目仓库：<a href="https://github.com/foresnailis/KittyToTiger">https://github.com/foresnailis/KittyToTiger</a></p><p>参考代码：<a href="https://www.csie.ntu.edu.tw/~b97074/vfx_html/hw1.html#C5">https://www.csie.ntu.edu.tw/~b97074/vfx_html/hw1.html#C5</a></p><p>python代码文件，需要opencv+numpy库。确保img文件夹下存有kitty.png/tiger.png两图片后，运行该代码，中间帧图片生成在result文件夹下。</p><p>本项目不支持自定义特征线、图片、中间帧数量，有兴趣可自行阅读修改代码。</p><h2 id="具体算法"><a href="#具体算法" class="headerlink" title="具体算法"></a>具体算法</h2><p>参考文献：<a href="http://www.cs.princeton.edu/courses/archive/fall00/cs426/papers/beier92.pdf">Feature-Based Image Metamorphosis , SIGGRAPH 1992</a></p><h3 id="特征线与像素点的关系"><a href="#特征线与像素点的关系" class="headerlink" title="特征线与像素点的关系"></a>特征线与像素点的关系</h3><p>首先，为两张图画上特征线，如图：</p><p><img src="https://zlisnail.cn/img/%E7%89%B9%E5%BE%81%E7%BA%BF.png" alt="特征线"></p><p>左右图相应位置上的特征线共同成对，共九对特征线。特征线的作用在于确定两张图的对应关系。当然，具体的操作需要落实到每个像素点上。那么，该如何确定两张图之间像素点的对应关系呢？</p><p>假设现在只设置一对特征线，要将线之间的关系延展到点之间的关系，需要用到uv两个变量。其中，v是点到特征线的距离，u是点到特征线的垂直落点在整条线段的位置，用比例表示。当两张图的两个点对于图中特征线有相同的uv值，则两个像素点是对应的。</p><p><img src="https://zlisnail.cn/img/single-line.jpg" alt="single-line"></p><p><img src="https://zlisnail.cn/img/u_v_math.jpg" alt="u_and_v"></p><p>而对于多对特征线，同一个点可能对应另一张图中的多个点，因此需要对这多个点加权，得到最终的对应点。权重公式如下：</p><p><img src="https://zlisnail.cn/img/multi-line.jpg" alt="multi-line"></p><p><img src="https://zlisnail.cn/img/weight.jpg" alt="weight"></p><p><img src="https://zlisnail.cn/img/image-20231002051739189.png" alt="image-20231002051739189"></p><h3 id="生成中间帧"><a href="#生成中间帧" class="headerlink" title="生成中间帧"></a>生成中间帧</h3><p>确定了两张图中像素与像素之间的对应关系之后，我们就可以生成中间帧了。对于中间帧，我们需要使用线性插值法，按照比例生成中间帧的特征线。换句话讲，比如我们要生成一个猫到虎的中间帧，这个中间帧上的特征线的两个顶点坐标、长度、角度，就由相应的猫图和虎图上对应的两条特征线上的顶点坐标、长度、角度相加除以二得到。这是生成一个中间帧的情况，程序中生成了5个中间帧，因此需要按照五分之一等分。</p><p>得到中间帧的特征线后，就可以找到中间帧上每个像素点分别在猫图和虎图上的对应像素点。两个像素点有两种色彩值（RGB+alpha），再利用线性插值原理混合它们，即可得到中间帧像素点的色彩值。</p><blockquote><p>找到两张图上的像素点后，可能会出现对应像素点的坐标为浮点数。按照道理来讲，数字图像在opencv中，每个像素点的坐标都是整数。如何确定浮点数坐标对应像素的色彩值呢？可以使用双线性插值法。如下图：</p><p><img src="https://zlisnail.cn/img/bilinear.jpg" alt="bilinear"></p></blockquote><h2 id="算法结果"><a href="#算法结果" class="headerlink" title="算法结果"></a>算法结果</h2><p><img src="https://zlisnail.cn/img/results_0.jpg" alt="results_0"><img src="https://zlisnail.cn/img/results_1.jpg" alt="results_1"><img src="https://zlisnail.cn/img/results_2.jpg" alt="results_2"><img src="https://zlisnail.cn/img/results_3.jpg" alt="results_2"><img src="https://zlisnail.cn/img/results_4.jpg" alt="results_4"></p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 算法实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAMES101-现代计算机图形学入门-笔记（三）</title>
      <link href="/2023/09/26/GAMES101-%E7%8E%B0%E4%BB%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%85%A5%E9%97%A8-%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/"/>
      <url>/2023/09/26/GAMES101-%E7%8E%B0%E4%BB%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%85%A5%E9%97%A8-%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="GAMES101-现代计算机图形学入门-笔记（三）"><a href="#GAMES101-现代计算机图形学入门-笔记（三）" class="headerlink" title="GAMES101-现代计算机图形学入门-笔记（三）"></a>GAMES101-现代计算机图形学入门-笔记（三）</h1><h2 id="深入了解变换"><a href="#深入了解变换" class="headerlink" title="深入了解变换"></a>深入了解变换</h2><h3 id="旋转变换的正交性质"><a href="#旋转变换的正交性质" class="headerlink" title="旋转变换的正交性质"></a>旋转变换的正交性质</h3><p><strong>如果一个矩阵的逆等于它的转置，则该矩阵称为正交矩阵</strong>。</p><p>考虑旋转变换矩阵。如果要旋转一个-θ角度（也即顺时针旋转θ角度），得到的矩阵正好是原矩阵的转置。同时，根据变换的意义，旋转-θ角度实际上也是旋转θ角度的矩阵的逆。因此，对于一个<strong>旋转变换矩阵，它是一个正交矩阵</strong>。</p><p><img src="https://zlisnail.cn/img/image-20230917212335228.png" alt="image-20230917212335228"></p><h3 id="三维变换"><a href="#三维变换" class="headerlink" title="三维变换"></a>三维变换</h3><h4 id="三维空间中的变换矩阵"><a href="#三维空间中的变换矩阵" class="headerlink" title="三维空间中的变换矩阵"></a>三维空间中的变换矩阵</h4><p>首先，类比二维空间，我们可以得到三维空间中的变换矩阵：</p><p><img src="https://zlisnail.cn/img/image-20230917212748038.png" alt="image-20230917212748038"></p><p>缩放与平移都不复杂，稍微难理解一点的是旋转矩阵。</p><p>绕轴旋转的变换矩阵如下：<br><img src="https://zlisnail.cn/img/image-20230917213242705.png" alt="image-20230917213242705"></p><p>注意，这里的所有变换矩阵，都有一个视角的问题。在二维空间，y轴正方向在x轴正方向右侧的情况下，逆时针旋转为正角度。那么扩展到三维空间，同样在xy平面上的旋转（也即绕z轴旋转），实际上我们应该从z轴正方向看向z轴负方向。对于其它平面上的旋转也是一样的。这实际上就是<strong>按照右手法则旋转</strong>。图形学中考虑旋转问题，基本都是默认右手法则。</p><h4 id="自由旋转"><a href="#自由旋转" class="headerlink" title="自由旋转"></a>自由旋转</h4><p>​    上面，我们学习了绕特定的轴的旋转变换该如何表示。那么在实际应用中，旋转的轴是自由的，并不局限在三个轴之内。如何去定义自由旋转呢？</p><p>​    直观地想象，我们可以用三个基础轴的旋转来表示自由旋转。如下图：</p><p><img src="https://zlisnail.cn/img/image-20230917214338829.png" alt="image-20230917214338829"></p><p>这种角称为欧拉角。</p><p>Rodrigue推导出了另一种，可以用一个<strong>单位</strong>方向向量表示旋转轴的方法：<br><img src="https://zlisnail.cn/img/image-20230917214510089.png" alt="image-20230917214510089"></p><p>在该式中，向量<strong>n</strong>表示一个从原点出发的<strong>单位</strong>向量，α表示绕该轴旋转的角度。用这个式子就可以推导出最终的变换矩阵写法。当然，如果轴并不经过原点，就需要先用平移变换使得向量经过原点。</p><p>有关该式子的推导，整体上的思路其实就是把旋转向量分解成与轴平行的向量以及与轴垂直的向量，然后平行向量不变，垂直向量旋转α角度。详细推导可见：<a href="https://www.cnblogs.com/wtyuan/p/12324495.html">罗德里格斯旋转公式（Rodrigues’ rotation formula）推导 </a>。很有意思，以后可以自己推一推。这个博客里的推导步骤非常详细，唯一没有提及的概念叫做<strong>投影矩阵</strong>。关于这个，可以在<a href="https://www.zhihu.com/question/40049682">一个向量乘它的转置，其几何意义是什么？</a>中找到。</p><p>另外，关于旋转，还有一种四元角的概念。它的目的是解决旋转角度变换不能直接通过旋转矩阵加减得到的问题。待了解。</p><h4 id="万向节锁"><a href="#万向节锁" class="headerlink" title="万向节锁"></a>万向节锁</h4><p>在大部分引擎当中，有一个有意思的现象叫做万向节锁。它的大概意思是说，物体的local坐标系中（local对应于global，世界坐标系），x,y,z三轴是嵌套的，例如在unity中，嵌套顺序就是y-&gt;x-&gt;z。当旋转物体的y轴时，物体的x、z轴会同步旋转；而当旋转物体的z轴时，y、x轴都不会发生任何的改变。这就导致，当你将物体绕着x轴旋转90度后，物体的z轴变了，但y轴没动，导致z和y轴重叠在了一起，在一条直线上。此时旋转物体的local坐标系中的y轴，会发现物体实际上在绕着物体local坐标系中的z轴旋转。图形化表示可见：<a href="https://www.bilibili.com/video/BV1o5411c7ex/?spm_id_from=333.337.search-card.all.click&amp;vd_source=dce5e3a4e8bded8fa669fa6355c5d0ed">万向锁详解</a> 。可以用陀螺仪来理解，详见：<a href="https://www.bilibili.com/video/BV1YJ41127qe/?spm_id_from=333.999.0.0&amp;vd_source=dce5e3a4e8bded8fa669fa6355c5d0ed">“欧拉角旋转”产生“万向锁”的来源，以及如何避免万向锁</a>。(PS:上述视频作者不是我，如有冒犯立马删除引用)</p><p>看到评论里有个解释很有意思，“之所以嵌套，而不是完全相对而言，主要是因为世界空间和物体本身的空间必须具备某种联系，在Unity中，这种联系是通过共用y轴实现的，这种共用仅仅体现在旋转上。当物体旋转时，对于y分量的旋转始终就是世界坐标系的y分量，而不会随着其他x、z分量的旋转而改变，这就是作者所讲的y排第一位的意义。如果这种旋转轴全部相对，或者就使用物体空间本身的轴，那么物体本身与世界空间就没有办法产生联系，我猜测，这是游戏引擎设计的时候所考虑的，无法避免的一种存在有缺陷的设计。 ”这里讲到的，世界空间与物体本身空间必须具备的某种联系，我不能完全理解。有待进一步探究。</p><h3 id="view-camera-transformation-视图变换"><a href="#view-camera-transformation-视图变换" class="headerlink" title="view/camera transformation 视图变换"></a>view/camera transformation 视图变换</h3><p>计算机中，要想将一个3D场景显示在屏幕上，需要设置两个位置：模型位置以及摄像机位置；最后，还需要通过投影变换，使得最终的屏幕上显示画面。下图用一个拍照的例子，生动形象地说明了这一过程：</p><p><img src="https://zlisnail.cn/img/image-20230919153600379.png" alt="image-20230919153600379"></p><p>上述三个变换，称为MVP变换。</p><p>如何定义一个相机的位置？用三个向量：</p><p><img src="https://zlisnail.cn/img/image-20230924220609432.png" alt="image-20230924220609432"></p><p>为了能让后续的一些操作更加简便，我们要将相机变换到原点，朝向z轴负方向（右手坐标系中），up向量指向y轴正方向。如图所示：</p><p><img src="https://zlisnail.cn/img/image-20230924221901385.png" alt="image-20230924221901385"></p><p>变换矩阵如下：</p><p><img src="https://zlisnail.cn/img/image-20230924222834178.png" alt="image-20230924222834178"></p><p>上面的M~view~也可以理解为基变换的过渡矩阵，可以将xyz坐标系中的物体变换到相机坐标系中去。上述公式中，g,t,(g×t)都是单位向量。</p><h3 id="Projection-transformation-投影变换"><a href="#Projection-transformation-投影变换" class="headerlink" title="Projection transformation 投影变换"></a>Projection transformation 投影变换</h3><p>分为正交投影（Orthographic projection）和透视投影（Perspective projection。</p><p><img src="https://zlisnail.cn/img/image-20230924232249147.png" alt="image-20230924232249147"></p><p>一个直观的区别就是，透视投影使得原先平行的两条线不再平行。下面的图例可以更好地展示它们的本质区别：</p><p><img src="https://zlisnail.cn/img/image-20230924232653880.png" alt="image-20230924232653880"></p><h4 id="正交投影"><a href="#正交投影" class="headerlink" title="正交投影"></a>正交投影</h4><p>一个简单的方法：在上述视图变换的基础上，扔掉z轴，并将物体在xy平面上的投影平移缩放到[-1,1]^2^上去。</p><p><img src="https://zlisnail.cn/img/image-20230924234039922.png" alt="image-20230924234039922"></p><p>标准做法：</p><p>首先将物体平移到原点上去，然后将它缩放到[-1,1]^3^（canonical cube）。<strong><em>为什么？</em></strong></p><p><img src="https://zlisnail.cn/img/image-20230924234222216.png" alt="image-20230924234222216"></p><p>从上面的图例中可以看出，所谓的far面的z值比near面的z值小，有点不太符合直觉。所以openGL使用左手系，从而保持far面z值更大。</p><h4 id="透视投影"><a href="#透视投影" class="headerlink" title="透视投影"></a>透视投影</h4><p>按照闫老师的方法，透视投影本质上是远面向近面上的投影，需要先将远面压缩到与近面相同的大小，然后应用正交投影。</p><p><img src="https://zlisnail.cn/img/image-20230925001515494.png" alt="image-20230925001515494"></p><p>没有完全理解，先把公式贴在这里：</p><p><img src="https://zlisnail.cn/img/image-20230925002544038.png" alt="image-20230925002544038"></p><p><img src="https://zlisnail.cn/img/image-20230925002604043.png" alt="image-20230925002604043"></p><p><img src="https://zlisnail.cn/img/image-20230925002615777.png" alt="image-20230925002615777"></p><p><img src="https://zlisnail.cn/img/image-20230925002645863.png" alt="image-20230925002645863"></p>]]></content>
      
      
      <categories>
          
          <category> 计算机图形学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机图形学 </tag>
            
            <tag> GAMES101 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAMES101-现代计算机图形学入门-笔记（二）</title>
      <link href="/2023/09/16/GAMES101-%E7%8E%B0%E4%BB%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%85%A5%E9%97%A8-%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2023/09/16/GAMES101-%E7%8E%B0%E4%BB%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%85%A5%E9%97%A8-%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="GAMES101-现代计算机图形学入门-笔记（二）"><a href="#GAMES101-现代计算机图形学入门-笔记（二）" class="headerlink" title="GAMES101-现代计算机图形学入门-笔记（二）"></a>GAMES101-现代计算机图形学入门-笔记（二）</h1><h2 id="变换transformation"><a href="#变换transformation" class="headerlink" title="变换transformation"></a>变换transformation</h2><h3 id="二维变换"><a href="#二维变换" class="headerlink" title="二维变换"></a>二维变换</h3><h4 id="Scale-缩放"><a href="#Scale-缩放" class="headerlink" title="Scale 缩放"></a>Scale 缩放</h4><p>​    均匀缩放变换：</p><p><img src="https://zlisnail.cn/img/image-20230916154811977.png" alt="缩放的矩阵变换"></p><p>不均匀缩放变换：</p><p><img src="https://zlisnail.cn/img/image-20230916154850850.png" alt="不均匀缩放变换"></p><p>对称变换：</p><p><img src="https://zlisnail.cn/img/image-20230916155032633.png" alt="image-20230916155032633"></p><p>​    切变shear matrix：</p><p>​    <img src="https://zlisnail.cn/img/image-20230916155338091.png" alt="切变变换"></p><h4 id="Rotate-旋转"><a href="#Rotate-旋转" class="headerlink" title="Rotate 旋转"></a>Rotate 旋转</h4><p>​     <img src="https://zlisnail.cn/img/image-20230916160705027.png" alt="image-20230916160705027"></p><p>上述变换均为线性变换</p><h4 id="Translation-平移"><a href="#Translation-平移" class="headerlink" title="Translation 平移"></a>Translation 平移</h4><p><img src="https://zlisnail.cn/img/image-20230916161449984.png" alt="image-20230916161449984"></p><p><img src="https://zlisnail.cn/img/image-20230916161641129.png" alt="image-20230916161641129"></p><blockquote><p>注意，在上图中，（x,y）是先经过线性变换，再平移。注意它的几何意义。</p></blockquote><p>平移变换不是线性变换，而是仿射变换。</p><p>附：线性变换的定义：<br><img src="https://zlisnail.cn/img/image-20230916163032476.png" alt="image-20230916163032476"></p><p>于是，为了将平移变换与其它变换的处理方式统一，人们增加了一个维度，引入了齐次坐标：</p><p><img src="https://zlisnail.cn/img/image-20230916163537144.png" alt="image-20230916163537144"></p><p>点与向量增加一个维度后，表示如下：</p><p><img src="https://zlisnail.cn/img/image-20230916163917737.png" alt="image-20230916163917737"></p><p>注意，在上面的第三个维度中，点与向量的表示方法不同。点的w值设置为1，向量的w值设置为0。这种表示方式，可以从很多个角度来理解。</p><p>比如，在平移变换中，点的平移确实会改变它的位置，即xy值；但向量平移，表示方式不能变，即xy不能变。如此，将w值设置为0，可以让向量在变换时屏蔽掉矩阵中的第三列，使得最终变换后的向量的坐标表示不受平移的影响。</p><p>从另一个角度，这种表示方式可以最大程度上满足点与向量加减法的数值解与实际意义之间的对应关系。考虑下图：</p><p><img src="https://zlisnail.cn/img/image-20230916165106083.png" alt="image-20230916165106083"></p><p>在这种表示方法下，向量加向量依旧能够得到一个向量；点减去点正好就是一个向量；点加上向量恰好可以得到一个点。需要特殊记忆的是点加点的算法。在齐次坐标表示法中，定义一个点的w值必须是1。也就是说，两个点的坐标直接相加后，因为w值为2，因此必须将xyw值同除以2，才能使得此时的结果xy正常表示点的坐标。换句话讲，齐次坐标表示下的点点相加，得到的点是两点连成线段的中点。</p><h4 id="齐次坐标-homogenous-coordinates"><a href="#齐次坐标-homogenous-coordinates" class="headerlink" title="齐次坐标 homogenous coordinates"></a>齐次坐标 homogenous coordinates</h4><p>​    上述所有变换可以统一到齐次坐标下。</p><p><img src="https://zlisnail.cn/img/image-20230916165735586.png" alt="image-20230916165735586"></p><p><img src="https://zlisnail.cn/img/image-20230916165845055.png" alt="image-20230916165845055"></p><p>注意，这里是在二维坐标下的仿射变换。它们的最后一行都是0，0，1。</p><h4 id="逆变换-inverse-transform"><a href="#逆变换-inverse-transform" class="headerlink" title="逆变换 inverse transform"></a>逆变换 inverse transform</h4><p>​    在数学上，逆变换就是原先仿射矩阵的逆矩阵。</p><p><img src="https://zlisnail.cn/img/image-20230916170241082.png" alt="image-20230916170241082"></p><h4 id="变换组合"><a href="#变换组合" class="headerlink" title="变换组合"></a>变换组合</h4><p>​    变换顺序是不能随便调换的；在数学意义上，矩阵乘法是没有交换律的。</p><p><img src="https://zlisnail.cn/img/image-20230916170559529.png" alt="image-20230916170559529"></p><p>如上图，注意变换是从右到左应用的。</p><p>也可以应用矩阵的乘法结合律，将所有简单的矩阵变换乘在一起，得到一个复杂的3×3矩阵，它表示将所有简单变换融合后的最终变换。</p><h4 id="绕特定点旋转的表示方法"><a href="#绕特定点旋转的表示方法" class="headerlink" title="绕特定点旋转的表示方法"></a>绕特定点旋转的表示方法</h4><p>​    由于旋转变换只能绕着原点，如果要绕着别的点旋转，必须先平移，将该点平移到原点的位置，再去旋转。</p><p>​    <img src="https://zlisnail.cn/img/image-20230916171454126.png" alt="image-20230916171454126"></p><h3 id="三维变换"><a href="#三维变换" class="headerlink" title="三维变换"></a>三维变换</h3><p>三维变换也需要平移，因此也需要使用齐次坐标。</p><p><img src="https://zlisnail.cn/img/image-20230916171830697.png" alt="image-20230916171830697"></p><p><img src="https://zlisnail.cn/img/image-20230916171924860.png" alt="image-20230916171924860"></p>]]></content>
      
      
      <categories>
          
          <category> 计算机图形学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机图形学 </tag>
            
            <tag> GAMES101 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAMES101-现代计算机图形学入门-笔记（一）</title>
      <link href="/2023/09/16/GAMES101-%E7%8E%B0%E4%BB%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%85%A5%E9%97%A8-%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2023/09/16/GAMES101-%E7%8E%B0%E4%BB%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%85%A5%E9%97%A8-%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="GAMES101-现代计算机图形学入门-笔记（一）"><a href="#GAMES101-现代计算机图形学入门-笔记（一）" class="headerlink" title="GAMES101-现代计算机图形学入门-笔记（一）"></a>GAMES101-现代计算机图形学入门-笔记（一）</h1><h2 id="一些基础概念"><a href="#一些基础概念" class="headerlink" title="一些基础概念"></a>一些基础概念</h2><p>实时：&gt;=30FPS      离线</p><p>右手坐标系：x轴方向向量叉乘y轴方向向量，结果向量与z轴方向一致</p><p>有些图形学API中，例如openGL，使用的是左手坐标系</p><p>向量叉乘满足乘法分配律</p><p><img src="https://zlisnail.cn/img/image-20230916140616597.png" alt="image-20230916140616597"></p><p><img src="https://zlisnail.cn/img/image-20230916140837379.png" alt="image-20230916140837379"></p><p>向量点乘可以判断两个向量在方向上的一致性。当两个向量的点乘结果大于0时，向量在方向上一定程度上同向；等于0时，向量垂直；小于0时，向量异向。</p><p>向量叉乘可以判断向量的左右关系。对于一个右手坐标系，有a、b两向量在xy平面上。如果a×b的结果向量指向z正方向，则说明b在a的左侧；如果指向负方向，则说明b在a的右侧。这里的左右，直观上判断，实际上就是以a的正方向与a的负方向为两个射线，将整个平面划分为两大块后，视角对准a的正方向，左与右也就随之确定。</p><p><img src="https://zlisnail.cn/img/image-20230916142039535.png" alt="ima-20230916142039535"></p><p>这种用向量叉乘判断左右的方法，可以用来判断一个像素点是否在一个三角形内部。上图右侧中，向量AB×向量AP，指向z轴正方向；同样的，向量BC×向量BP，以及向量CA×向量CP，均指向z轴正方向。这也就说明了三个以P为终点的向量，均在三角形对应三条边的左侧，也就意味着P点在三角形的内部。当然，这是在ABC三个顶点逆时针排列的情况下。当三个顶点顺时针排列时，同样的三次叉乘，它们的结果都应该指向z轴的反方向，才能表示P点在三角形内部。不管三个顶点怎样排列，判断一个点是否在一个三角形面中的方法很简单，只需要判断三次叉乘的结果，是否均指向同一个方向。</p><p>三维空间中，任意向量在任意三维直角坐标系中的投影计算方法： 与三轴的单位正向量点乘，得到投影。如下图：</p><p><img src="https://zlisnail.cn/img/image-20230916142945775.png" alt="image-20230916142945775"></p>]]></content>
      
      
      <categories>
          
          <category> 计算机图形学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机图形学 </tag>
            
            <tag> GAMES101 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记（三）--值迭代与策略迭代</title>
      <link href="/2023/09/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89-%E5%80%BC%E8%BF%AD%E4%BB%A3%E4%B8%8E%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3/"/>
      <url>/2023/09/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89-%E5%80%BC%E8%BF%AD%E4%BB%A3%E4%B8%8E%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="强化学习笔记（三）—值迭代与策略迭代"><a href="#强化学习笔记（三）—值迭代与策略迭代" class="headerlink" title="强化学习笔记（三）—值迭代与策略迭代"></a>强化学习笔记（三）—值迭代与策略迭代</h1><p>首先，再次复习一下贝尔曼公式的matrix-vector数学形式：</p><script type="math/tex; mode=display">v=f(v)=\mathop{max}\limits_{π}(r_π+γP_πv)</script><h2 id="value-iteration-algorithm"><a href="#value-iteration-algorithm" class="headerlink" title="value iteration algorithm"></a>value iteration algorithm</h2><p>值迭代方法，实际上就是在强化学习笔记（二）中提到的迭代算法。数学表达式如下：</p><script type="math/tex; mode=display">v_{k+1}=f(v_k)=\mathop{max}\limits_{π}(r_π+γP_πv_k),\ \ \ \ \ \ \ \ \  k=1,2,3...</script><p>在（二）中，我们已经证明了，利用这样的迭代算法，最终可以得到一个不动点<code>v*</code>，作为最优的state-value。这个算法可以分为两个步骤：</p><h4 id="policy-update"><a href="#policy-update" class="headerlink" title="policy update"></a>policy update</h4><p>​    <img src="https://zlisnail.cn/img/image-20230915205055682.png" alt="image-20230915205055682"></p><p>当然，（二）中，我们已经推导出，π就是贪婪策略</p><h4 id="value-update"><a href="#value-update" class="headerlink" title="value update"></a>value update</h4><p>​    <img src="https://zlisnail.cn/img/image-20230915205125411.png" alt="image-20230915205125411"></p><p>注意，在第二步value update中展示的式子，并不是贝尔曼公式。贝尔曼公式中，等号左右两式的v是相同的v，是state-value；而这里，左右两式中的v仅仅是值，它们不相等，且没有任何特殊含义，更不是state-value。</p><p>上述步骤伪代码如下：</p><p><img src="https://zlisnail.cn/img/image-20230915212311986.png" alt="值迭代伪代码"></p><h2 id="policy-iteration-algorithm"><a href="#policy-iteration-algorithm" class="headerlink" title="policy iteration algorithm"></a>policy iteration algorithm</h2><p>上面的值迭代算法中，每次迭代都会得到一个新的v~k~值。而在策略迭代算法中，每次迭代都会得到一个新的策略π~k+1~，这个策略是基于由π~k~生成的贝尔曼公式的解v~k~得来的，依旧应用贪婪的思想。具体数学表达式如下：</p><h4 id="policy-evaluation"><a href="#policy-evaluation" class="headerlink" title="policy evaluation"></a>policy evaluation</h4><p>​    <img src="https://zlisnail.cn/img/image-20230916230045158.png" alt="image-20230916230045158"></p><p>这一步实际上是基于π~k~求解贝尔曼公式。如学习笔记（一）中所说，简单的公式可以直接使用线性方程组解出；复杂的可以使用迭代算法。</p><h4 id="policy-improvement"><a href="#policy-improvement" class="headerlink" title="policy improvement"></a>policy improvement</h4><p>​    <img src="https://zlisnail.cn/img/image-20230916230119101.png" alt="image-20230916230119101"></p><p>这里的π~k+1~一定是优于π~k~的，这个证明在赵老师的书中可以找到。（这次就先略过）</p><p>当然，依旧是贪婪策略。</p><p>整体过程如下：</p><p><img src="https://zlisnail.cn/img/image-20230916230204055.png" alt="image-20230916230204055"></p><p>整个迭代过程，最终一定能够收敛到最优策略、最优state-value。证明同样在赵老师的书中。（略过）</p><p>伪代码如下：</p><p><img src="https://zlisnail.cn/img/image-20230916230857999.png" alt="image-20230916230857999"></p><h4 id="truncated-policy-iteration"><a href="#truncated-policy-iteration" class="headerlink" title="truncated policy iteration"></a>truncated policy iteration</h4><p>仔细比对上面的两个算法，发现其实它们在很大部分上相同：</p><p><img src="https://zlisnail.cn/img/image-20230916231615533.png" alt="image-20230916231615533"></p><p> 不同的是，在求解下一个迭代的v值时，值迭代算法仅计算了一次（没有迭代），而策略迭代算法则不断迭代v直至收敛。如下图：</p><p><img src="https://zlisnail.cn/img/image-20230916231825941.png" alt="image-20230916231825941"></p><p>截断策略迭代算法，既避免策略迭代算法中求解贝尔曼公式时迭代次数过多的问题，也防止因为值迭代算法在每次迭代中计算v值时过于简单导致整体迭代次数过多的问题。</p><p>因此，截断策略迭代算法的代码几乎与策略迭代算法相同，唯一不同的在于求解贝尔曼公式时设置了一个最大迭代次数。（原先是判断误差是否小于某数）</p><p><img src="https://zlisnail.cn/img/image-20230916232124939.png" alt="image-20230916232124939"></p><p>这个算法一定是可以收敛的，证明同样在赵老师的书中。以后再看吧（汗，多少证明没看-_-||）。</p><p>直观的三种算法的比较如下图，其中横坐标是迭代次数。</p><p><img src="https://zlisnail.cn/img/image-20230916232419214.png" alt="image-20230916232419214"></p><p>伪代码中j~truncated~的值对收敛速度的影响：</p><p><img src="https://zlisnail.cn/img/image-20230916232522198.png" alt="image-20230916232522198"></p><p>可以看到，设置适当的j~truncated~值是必要的。</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> 强化学习数学原理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记（二）--贝尔曼最优公式</title>
      <link href="/2023/09/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89-%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F/"/>
      <url>/2023/09/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89-%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="强化学习笔记（二）—贝尔曼最优公式"><a href="#强化学习笔记（二）—贝尔曼最优公式" class="headerlink" title="强化学习笔记（二）—贝尔曼最优公式"></a>强化学习笔记（二）—贝尔曼最优公式</h1><p>贝尔曼最优公式（Bellman optimality equation,BOE）实际上就是利用贝尔曼公式，求解使得所有state的v值最大的策略π，以及相应的v值。数学上，只需要在贝尔曼公式的右式添加一个max~π~即可。也就是说，贝尔曼公式，对应一个策略π，它的策略是给定的，非变量；而贝尔曼最优公式，则是将π值作为变量，需要找到最优的π*。</p><p>从另一个角度理解，也可以讲，贝尔曼最优公式是一种特殊的贝尔曼公式，它的策略就是π<em>，得到的v值就是最优值v\</em>。</p><p>求解BOE是一个最优化问题。可以按照下面的思路进行求解π值：</p><p><img src="https://zlisnail.cn/img/image-20230914212226037.png" alt="贝尔曼最优公式"></p><p>于是，最优策略实际上就是使得每个s处都取q值最大action的策略π。（<strong>贪婪</strong>策略）</p><p><img src="https://zlisnail.cn/img/image-20230914212405923.png" alt="贪婪策略"></p><p>有关这个策略是否能得到最优值，还有一个定理可以证明。定理如下，证明在赵老师的书里。（话说为什么还要再证明一次？）</p><p><img src="https://zlisnail.cn/img/image-20230914232905082.png" alt="最优策略定理"></p><p>上面的结果，给出了求最优值时的具体策略，即π值。于是，π的值确定。那么我们要如何求解v值呢？变换贝尔曼最优公式，我们可以得到如下式子：</p><p><img src="https://zlisnail.cn/img/image-20230914212838417.png" alt="化简贝尔曼最优公式到v=f(v)的形式"></p><h4 id="contraction-mapping-收缩映射"><a href="#contraction-mapping-收缩映射" class="headerlink" title="contraction mapping 收缩映射"></a>contraction mapping 收缩映射</h4><p><img src="https://zlisnail.cn/img/image-20230914222955819.png" alt="contraction mapping"></p><h4 id="fixed-point-不动点"><a href="#fixed-point-不动点" class="headerlink" title="fixed point 不动点"></a>fixed point 不动点</h4><p>​    <img src="https://zlisnail.cn/img/image-20230914223020975.png" alt="fixed point"></p><h4 id="contraction-mapping-theorem-收缩映射定理"><a href="#contraction-mapping-theorem-收缩映射定理" class="headerlink" title="contraction mapping theorem 收缩映射定理"></a>contraction mapping theorem 收缩映射定理</h4><p>​    该定理未给出证明，之后可以研究下：</p><p><img src="https://zlisnail.cn/img/image-20230914223309271.png" alt="contraction mapping theorem"></p><p>同时，我们也可以证明，贝尔曼最优公式就是一个收缩映射（证明待查）：<br><img src="https://zlisnail.cn/img/image-20230914223743541.png" alt="贝尔曼最优公式是一个收缩映射"></p><p>于是，应用收缩映射定理，我们可以轻易得到，贝尔曼最优公式的迭代算法：</p><p><img src="https://zlisnail.cn/img/image-20230914224340807.png" alt="贝尔曼最优公式应用收缩映射定理"></p><p>换句话讲，现在我们清楚地知道，贝尔曼最优方程有且仅有唯一的不动点v，且可以通过不断迭代得到。很明显，这个不动点，就是BOE的解。（注意，最优的v是唯一的，但是最优的策略π不一定）</p><h4 id="value-iteration-algorithm"><a href="#value-iteration-algorithm" class="headerlink" title="value iteration algorithm"></a>value iteration algorithm</h4><p><img src="https://zlisnail.cn/img/image-20230914225730112.png" alt="程序算法"></p><h4 id="奖励对策略的影响"><a href="#奖励对策略的影响" class="headerlink" title="奖励对策略的影响"></a>奖励对策略的影响</h4><p>对r作仿射变换，即r-&gt;ar+b，最终策略不会有什么变化。不同状态r值之间的相对关系才是决定策略的关键因素。</p><p> <img src="https://zlisnail.cn/img/image-20230915171837719.png" alt="奖励的线性变换对v的影响"></p><p>由上图可以看到，BOE中的因子式</p><script type="math/tex; mode=display">γP_πv</script><p>也会被仿射变换，而贪心策略一直都在选择最大q值的动作，因此最终策略不会有变化。</p><p>（这里没有说明a能否小于0。如果a小于0，原本q值最大的动作，反而成为最小的，这不就使得策略改变吗？待讨论）</p><p>最后，再对γ进行一些补充讨论：</p><p><img src="https://zlisnail.cn/img/image-20230915174613099.png" alt="一个例子"></p><p>在上图中，左边是最优策略，右边是非最优的。前面我们知道，所谓最优，指的是每个v值都是最大的，换句话说，从每个状态s出发的轨迹，得到的累积奖励return都是最大的。为什么（1,2）处的state-value不是最优的v值呢？因为它在此处的策略是向左走的。向左走，导致它的return式子为下图b式：</p><p><img src="https://zlisnail.cn/img/image-20230915175032973.png" alt="两种策略的return值"></p><p>显然，由于γ的存在，路径越长的轨迹，越不容易获得高的return。于是，继续迭代BOE，我们就会得到结果a。这也给我们设计奖励带来启示，不需要可以设置惩罚（即负数的奖励），有时候直接设计为0即可。</p><p>​    </p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> 强化学习数学原理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记（一）--贝尔曼公式</title>
      <link href="/2023/09/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%85%AC%E5%BC%8F/"/>
      <url>/2023/09/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%85%AC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h4 id="贝尔曼公式学习"><a href="#贝尔曼公式学习" class="headerlink" title="贝尔曼公式学习"></a>贝尔曼公式学习</h4><p>​    trajectory（轨迹）的return（返回值），用于衡量某策略的优劣，即一个状态转移链条的最终reward值。</p><p>​    计算过程中，对于长度较长的trajectory，一般有γ作为折扣因子。折扣因子较小，表示策略重视近期效果；折扣因子较大，表示策略重视远期效果。</p><p>​    观察下面的例子：</p><p><img src="https://zlisnail.cn/img/image-20230907161312156.png" alt="image-20230907161312156"></p><p>这里，value~i~的定义是从s~i~出发的无限长度轨迹的return值。</p><p>上述情况，表明一个状态的value，依赖于其他状态的value。</p><p>于是，利用线性代数的知识，就可以解出该方程组，获得所有的value值：</p><p><img src="https://zlisnail.cn/img/image-20230907164151117.png" alt="image-20230907164151117"></p><p> 上面是对核心概念的理解，做了简化。下图则定义了强化学习最常用的几个变量。注意，大写的S/A/R，它们都是<strong>随机变量（random variables）</strong>。随机变量可以通过概率公式求出，可以对他们求期望等。</p><p><img src="https://zlisnail.cn/img/image-20230907170454906.png" alt="image-20230907170454906"></p><p>​      由此，将多步连接起来，就得到了一个trajectory。在这个轨迹中，使用<em>G~t~</em>来表示discounted return。 于是，我们可以定义一个重要的变量：state-value function（简称state-value）。</p><p><img src="https://zlisnail.cn/img/image-20230907171058820.png" alt="image-20230907171058820"></p><p>由定义公式可见，该函数的自变量包括当前状态s和策略π。</p><p>区别return与state value，return 是单个轨迹的总奖励值；而state value是从当前状态出发按照一定的策略，可能得到的所有轨迹的奖励值的<strong>期望</strong>。</p><p>由上述定义公式，结合trajectory的概念，可以轻易地得出下式：</p><p> <img src="https://zlisnail.cn/img/image-20230911212029725.png" alt="image-20230911212029725"></p><p>其中，第一项的计算方法：</p><p><img src="https://zlisnail.cn/img/image-20230912145856573.png" alt="image-20230912145856573"></p><p>第二项的计算方法：</p><p><img src="https://zlisnail.cn/img/image-20230912150431461.png" alt="image-20230912150431461"></p><p>由此得到贝尔曼公式：</p><p><img src="https://zlisnail.cn/img/image-20230912150850381.png" alt="image-20230912150850381"></p><p>所有s都有一个这样的式子，将所有式子联立，就可以得到一个方程组，解出所有的v。这个式子中，π就是策略；两个p就是环境模型。解出这个式子，得到v值，实际上也就是在<strong>评估一个策略</strong>（policy evaluation）。（从v的下标为π中，我们也可以看出，π是这个式子的关键）。</p><p>分析这个式子，可以看到，state-value首先根据不同的动作去分类，然后分别考虑每个动作获得的即时奖励和长远奖励（用下一个状态的state-value来表示，state-value可以理解为一个状态的价值；价值越高，未来越有可能获得高奖励 ）。</p><p>上述式子便于理解，但是是独立的。下面，我们来推导出贝尔曼公式的矩阵形式。</p><p>首先，对上式进行简化，提炼出该式子的核心：即时奖励与长远奖励。</p><p><img src="https://zlisnail.cn/img/image-20230912154047641.png" alt="image-20230912154047641"></p><p>将上式中的v\r\P，替换成向量或矩阵，就可以得到：</p><p><img src="https://zlisnail.cn/img/image-20230912154850369.png" alt="image-20230912154850369"></p><p>注意，上图里，大写字母P代表状态转移矩阵。矩阵有一种下标表示的写法，值得学习。另外，在定义向量的时候，规定向量每个元素的范围的写法，也非常有意思；R^n^既可以表示向量的长度，也可以表示向量元素的取值范围。别忘了将向量转置。</p><p>例子：</p><p><img src="https://zlisnail.cn/img/image-20230912165518172.png" alt="image-20230912165518172"></p><p>由上面的贝尔曼方程，我们可以轻易得到一种v的解法，那就是：</p><p><img src="https://zlisnail.cn/img/image-20230912175304959.png" alt="image-20230912175304959"></p><p>但是该解法需要对矩阵求逆（该矩阵一定是有逆的，因为当γ小于1时，该矩阵为严格对角占优矩阵），当状态空间非常大时，求逆运算会非常复杂。于是，有以下迭代算法：</p><p><img src="https://zlisnail.cn/img/image-20230912175418012.png" alt="image-20230912175418012"></p><p>其中，v~0~即为r~π~。当k趋于无穷时，v~k~无穷接近于v~π~，证明如下：</p><p><img src="https://zlisnail.cn/img/image-20230912175637624.png" alt="image-20230912175637624"></p><h5 id="action-value"><a href="#action-value" class="headerlink" title="action value"></a>action value</h5><p>​    直译作动作价值，实际上当前状态也是因子：</p><p><img src="https://zlisnail.cn/img/image-20230912224228146.png" alt="image-20230912224228146"></p><p>​    定义式子如下，用q~π~来表示：</p><p><img src="https://zlisnail.cn/img/image-20230912224645939.png" alt="image-20230912224645939"></p><p>从这个图中就可以看到，实际上q~π~（s,a）就是将v~π~(s)中的因子替换掉（即时奖励+长久奖励）：</p><p><img src="https://zlisnail.cn/img/image-20230912224916522.png" alt="image-20230912224916522"></p><p>至此，本节结束。贝尔曼公式是整个强化学习的基础，后续会多次用到。若未来对贝尔曼公式有更深的理解，该笔记将继续更新。</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> 强化学习数学原理 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
