<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>QT5命令重做、撤销框架</title>
      <link href="/2023/11/01/QT5%E5%91%BD%E4%BB%A4%E9%87%8D%E5%81%9A%E3%80%81%E6%92%A4%E9%94%80%E6%A1%86%E6%9E%B6/"/>
      <url>/2023/11/01/QT5%E5%91%BD%E4%BB%A4%E9%87%8D%E5%81%9A%E3%80%81%E6%92%A4%E9%94%80%E6%A1%86%E6%9E%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="QT5命令重做、撤销框架"><a href="#QT5命令重做、撤销框架" class="headerlink" title="QT5命令重做、撤销框架"></a>QT5命令重做、撤销框架</h1><h2 id="简单介绍"><a href="#简单介绍" class="headerlink" title="简单介绍"></a>简单介绍</h2><h3 id="命令模式"><a href="#命令模式" class="headerlink" title="命令模式"></a>命令模式</h3><p>命令模式属于行为型模式，调用者通过调用命令对象，来向接受者传达命令。</p><p>通过把命令或者说请求封装成一个对象，可以实现命令调用者与命令接受者之间的解耦，从而可以使用不同的请求参数化其他对象，并且能够支持请求的排队执行、记录日志、撤销等（附加控制）功能。</p><p>主要应用场景：控制命令的执行，比如异步、延迟、排队执行命令、撤销重做命令、存储命令、给命令记录日志等等。</p><p>命令模式最核心的实现手段，就是将函数封装成对象。</p><p><img src="https://zlisnail.cn/img/image-20231031174209012.png" alt="image-20231031174209012"></p><h3 id="备忘录模式"><a href="#备忘录模式" class="headerlink" title="备忘录模式"></a>备忘录模式</h3><p>备忘录模式是另一个行为型模式，在<strong>不违背封装原则</strong>的前提下，捕获一个对象的内部状态，并在该对象之外<strong>保存这个状态</strong>，以便之后恢复对象为先前的状态。</p><p>与命令模式相比，它的应用场景就比较有限，主要用来防止丢失、撤销、恢复。 </p><p>对于大对象的备份来说，备份占用的存储空间会比较大，备份和恢复的耗时会比较长。针对这个问题，不同的业务场景有不同的处理方式。比如，只备份必要的恢复信息，结合最新的数据来恢复；再比如，全量备份和增量备份相结合，低频全量备份，高频增量备份，两者结合来做恢复。</p><p><img src="https://zlisnail.cn/img/image-20231031191704642.png" alt="image-20231031191704642"></p><h2 id="QT5中的撤销-重做框架"><a href="#QT5中的撤销-重做框架" class="headerlink" title="QT5中的撤销/重做框架"></a>QT5中的撤销/重做框架</h2><p>下面我来介绍一下QT5当中的命令撤销重做框架，它就是命令模式与备忘录模式的结合，一般用于如画板、文档编辑等工作区。下图是一个画板的示例程序，用户可以在画板上添加、删除或者移动组件，这三个功能是围绕着QUndoCommand和QUndoStack来展开的。</p><p>QUndoCommand和QUndoStack是QT5的命令撤销重做框架中最重要的两个类。一方面，QUndoCommand作为命令模式的直接应用，它是一个抽象基类，新建、删除、移动的命令都是它的子类，他们由MainWindow调用，直接作用于画板DiagramScene之上；另一方面，QUndoStack可以看做备忘录模式中的管理者，QUndoCommand可以看做备忘录，每当MainWindow调用一个QUndoCommand时都会将它push到QUndoStack当中，QUndoStack维护一个QUndoCommand的列表与指向当前Command的指针。</p><p>左图中的Command List就展示了QUndoStack中的内容。当前的状态是Move Triangle at (178,66)之后的状态。点击相应的命令就可以切换到对应的命令执行后的状态。这个视图是由QUndoView自动生成的。</p><p><img src="https://zlisnail.cn/img/image-20231101191626302.png" alt="image-20231101191626302"></p><hr><p>下面我们针对源代码进行讲解。</p><p>QUndoCommand最重要的就是undo和redo函数，它必须由子类实现。undo函数表示撤销命令的操作，redo函数表示重做命令的操作。</p><p>可以看到，在画板上生成正方形的addBox函数就是通过实例化addCommand函数，然后直接push到undoStack当中来实现的。通过undoStack，来管理何时撤销、何时重做该命令。</p><p><img src="https://zlisnail.cn/img/image-20231101192403937.png" alt="image-20231101192403503"></p><hr><p>由于D指针设计模式，QT的QUndoStackPrivate保存着QUndoStack的主要成员变量。</p><p>在左图中可以看到，所有命令存放在一个列表command_list当中，通过指针index来表示当前的状态。此外还有一些变量，例如indo_limit规定了栈的大小；clean_index指定了当前clean state所在的状态；group指向当前命令栈所属的栈群；macro_stack表示宏命令栈。下面我来依次讲解。</p><p>每次push函数调用时都会检查是否栈溢出，在checkUndoLimit中就可以看到，当del_limit大于等于0时，就会从栈底去除命令。</p><p><img src="https://zlisnail.cn/img/image-20231101193106771.png" alt="image-20231101193106771"></p><hr><p>当一个应用程序有多个工作区时，每个工作区都会对应一个QUndoStack。当存在多个栈时，一个命令应该push到哪个栈上，或者说Ctrl+Z应该作用在哪个栈上呢？</p><p>QUndoGroup就可以维护一群QUndoStack，在同一时间只有一个QUndoStack在起作用，可以看到有一个stack_list和一个active指针指向处于active状态的栈。所有的unde/redo/push都会所用在当前active的stack当中。</p><p><img src="https://zlisnail.cn/img/image-20231101193010326.png" alt="image-20231101193010326"></p><hr><p>clean state 用于标定当前存贮在磁盘上的文件状态。当用户修改文件但还没有CtrlS保存的时候，文档就离开了clean state。在新状态保存就会重置clean state到当前命令。</p><p>clean state是由QUndoStack的clean_index来维护的，栈指针进入或者离开该状态时都会发出信号，编写相应的槽函数就可以实现类似于vscode的保存提示UI。可以看到vsc窗口中有两个tab，有圆点的这个表示对文件进行了修改，不处于clean state；而保存过后圆点消失，就如同右边的tab一样。</p><p><img src="https://zlisnail.cn/img/image-20231101194254460.png" alt="image-20231101194254460"></p><hr><p>当命令粒度过小时，会出现命令对象声明过多、内存爆炸的现象；或者是命令过于繁琐。QT提供两种命令合并方法，Command Merge直接将stack中的多个命令融合为一个；Macro Command则将多个子命令融合为一个父命令，在执行时共同执行，但仍然分开存储。它们的存储粒度不同，适用的情景也不同。</p><p><img src="https://zlisnail.cn/img/image-20231101194712749.png" alt="image-20231101194712749"></p><hr><p>最后进行总结；QT5的命令框架中，命令模式的运用将调用者与接受者解耦，从而可以使得命令参数化，对命令的操作更灵活；但是也由于对命令的封装，可能导致具体的命令类太多太庞杂，需要结合特定情景使用。</p><p>而备忘录模式的结合保存了每个命令的redo和undo操作，可以实现灵活的状态控制。Git版本控制系统就与它相似。但是命令粒度太小太多时可能出现内存问题，可以通过多命令组合的方式解决。</p><p><img src="https://zlisnail.cn/img/image-20231101195047306.png" alt="image-20231101195047306"></p>]]></content>
      
      
      <categories>
          
          <category> Cpp实践 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> QT </tag>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记（五）-随机近似与随机梯度下降</title>
      <link href="/2023/10/17/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E9%9A%8F%E6%9C%BA%E8%BF%91%E4%BC%BC%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
      <url>/2023/10/17/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E9%9A%8F%E6%9C%BA%E8%BF%91%E4%BC%BC%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
      
        <content type="html"><![CDATA[<h1 id="强化学习笔记（五）—随机近似与随机梯度下降"><a href="#强化学习笔记（五）—随机近似与随机梯度下降" class="headerlink" title="强化学习笔记（五）—随机近似与随机梯度下降"></a>强化学习笔记（五）—随机近似与随机梯度下降</h1><p>为了能够更好地理解下一章要讲的时序差分方法(temporal-difference)，本章作为补充知识，弥补上一章与下一章之间的知识鸿沟。</p><hr><h2 id="mean-estimation"><a href="#mean-estimation" class="headerlink" title="mean estimation"></a>mean estimation</h2><p>在强化学习方法中，有很多时候需要用到<strong>mean estimation</strong>，即用x的平均数估计x的期望。传统方法中，需要收集到所有采样后再求平均，造成计算效率不高。下面，我们研究一种增量迭代的方法，能够在采样的同时，更新x的平均数，从而能同步估计x的期望值。</p><p>首先，传统方法如下：</p><p><img src="https://zlisnail.cn/img/image-20231010163338611.png" alt="image-20231010163338611"></p><p>新方法实际上就是研究了k次采样的平均数和k+1次采样平均数之间的关系：<br><img src="https://zlisnail.cn/img/image-20231010163551118.png" alt="image-20231010163551118"></p><p>迭代过程如下：</p><p><img src="https://zlisnail.cn/img/image-20231010163713358.png" alt="image-20231010163713358"></p><p>要求得w~k+1~，需要两个量，w~k~和k。现在，我们对这个式子进行处理：</p><p><img src="https://zlisnail.cn/img/image-20231010164027648.png" alt="image-20231010164027648"></p><p>当α满足一定条件时，这个递推公式也可以估计x的期望。这就涉及到下面的Robbins-Monro algorithm。</p><h2 id="Robbins-Monro-algorithm"><a href="#Robbins-Monro-algorithm" class="headerlink" title="Robbins-Monro algorithm"></a>Robbins-Monro algorithm</h2><p>随机采样（<strong>stochastic approximation</strong>，<strong>SA</strong>）方法，指代一类方法，它们会对随机变量进行采样，并通过迭代的方法进行方程的求解或者解决优化问题。相比于其它的求解方法，SA的优势在于不需要知道目标函数的表达式或是导数。Robbins-Monro算法就是一个SA方法，它是SA领域具有开创性质的成果；而随机梯度下降方法则是Robbins-Monro算法的一种特殊形式。下面，我们对Robbins-Monro算法进行进一步了解。</p><p>Robbins-Monro算法解决的问题形式很简单：<br><img src="https://zlisnail.cn/img/image-20231010165601814.png" alt="image-20231010165601814"></p><p>这之中，w可以是向量，也可以是标量。简单的形式往往蕴含巨大的能量，许多问题都可以化简为这样的形式；例如最优化问题，它需要求解极值点，而所谓的极值点实际上就是梯度为0的点，就可以化为这种形式。</p><p>当函数g的表达式已知时，我们有很多种方法来求解这个方程；而当g的表达式未知时，比如一个复杂的神经网络，我们要求解一个输入w，使得神经网络的输出为0，那么我们就可以用Robbins-Monro算法来求解。</p><h3 id="RM算法"><a href="#RM算法" class="headerlink" title="RM算法"></a>RM算法</h3><p>要求解$g(w^*)=0$，RM算法的求解方法如下：</p><p><img src="https://zlisnail.cn/img/image-20231014204206261.png" alt="image-20231014204206261"></p><p>其中，w~k~是第k次迭代得到的值，$\widetilde{g}$是对g的噪音观测值，a~k~是一个系数。记住，在RM算法中，函数g的表达式是不知道的。<strong>没有模型时，就需要数据。</strong></p><p>RM算法需要一些约束条件：</p><p><img src="https://zlisnail.cn/img/image-20231014205327290.png" alt="image-20231014205327290"></p><blockquote><p>术语with probability 1(w.p.1)的意思是，在概率意义上必定发生。在这里，w不是普通的数，它是带有概率的观测值；因此，用w.p.1表示一定会发生。这个术语在RL（reinforcement learning）中也会经常出现。</p></blockquote><p>仔细观察上面的约束条件。</p><p>第一个条件，对函数g的梯度进行了要求。这个约束保证了函数单调递增、梯度有界、一定有零点。</p><blockquote><p>对于凸函数来说，我们要求它的最小值，也就是梯度为0的点。我们将它的梯度视作另一个新函数，这个函数的梯度（即hessian矩阵）一定是满足这个条件的，于是我们就可以用RM求解凸函数最优值。</p><p>梯度有界是为了保证不会出现函数突然快速升高到无穷的情况，在这种情况下，在无穷附近取w值容易发散，收敛不到w^*^。</p></blockquote><p>第二个条件，对系数a~k~进行了要求。保证了a~k~收敛到0的同时，确保a~k~不会收敛地太快，防止它在有限步内就会收敛到0。</p><blockquote><p>保证a~k~能收敛到0，是为了让最后w~k+1~和w~k~能够相互靠近，越来越近，直到收敛到w~k~。而确保a~k~不会收敛地太快，有如下解释：</p><p><img src="https://zlisnail.cn/img/image-20231014213245056.png" alt="image-20231014213245056"></p><p>满足上述两个条件最常见的a~k~序列就是$a_k=\frac{1}{k}$ 。证明如下：</p><p><img src="https://zlisnail.cn/img/image-20231014213729302.png" alt="image-20231014213729302"></p></blockquote><p>第三个条件，对测量误差η~k~进行了要求。常见的情况是η~k~满足独立同分布条件(iid)。</p><p>回到上一节我们最后提出的问题，满足怎样条件的α~k~的mean estimation算法能够收敛？然后我们发现，α~k~就是RM算法中的系数a~k~。</p><p><img src="https://zlisnail.cn/img/image-20231014214158939.png" alt="image-20231014214158939"></p><p>这个算法转化为RM算法的形式，如下：</p><p><img src="https://zlisnail.cn/img/image-20231014215021827.png" alt="image-20231014215021827"></p><p>RM定理之外还有一个更广泛的定理，在此列出：</p><p><img src="https://zlisnail.cn/img/image-20231014215236491.png" alt="image-20231014215236491"></p><h3 id="Stochastic-gradient-descent-随机梯度下降"><a href="#Stochastic-gradient-descent-随机梯度下降" class="headerlink" title="Stochastic gradient descent 随机梯度下降"></a>Stochastic gradient descent 随机梯度下降</h3><p>SGD是RM算法的一个特殊情况。它也可以用来解释我们第一部分介绍的mean estimation算法。</p><h4 id="SGD解决的问题与形式"><a href="#SGD解决的问题与形式" class="headerlink" title="SGD解决的问题与形式"></a>SGD解决的问题与形式</h4><p>SGD解决的问题描述如下：</p><p><img src="https://zlisnail.cn/img/image-20231017205538985.png" alt="image-20231017205538985"></p><p>观察到上述问题描述中，一个重要的特征是有一个随机变量X，解决的问题是要针对X求使得f期望最小的w。针对这样的问题，可以有如下算法：</p><p>gradient descent GD 梯度下降:<br><img src="https://zlisnail.cn/img/image-20231017210057215.png" alt="image-20231017210057215"></p><p>梯度下降直接明了，但是由于X是随机变量，它的期望值得到；期望值直接由模型得到，即数学推导。</p><p>batch gradient descent BGD 批量梯度下降：</p><p><img src="https://zlisnail.cn/img/image-20231017212621893.png" alt="image-20231017212621893"></p><p>其实就是在GD的基础上做改进，通过大数定理来用数据取代模型。它会对X所有可能的取值采样，或者利用X的所有采样值，然后求平均，可能面临需要很多采样的问题。</p><p>而SGD则直接用一次采样来估计期望值：</p><p><img src="https://zlisnail.cn/img/image-20231017212933807.png" alt="image-20231017212933807"></p><h4 id="SGD收敛性原理"><a href="#SGD收敛性原理" class="headerlink" title="SGD收敛性原理"></a>SGD收敛性原理</h4><p>证明SGD收敛性，可以通过将SGD化为一个RM算法，然后利用RM算法收敛性来证明。</p><p>因为SGD是对GD的改进，是利用一次采样估计总体期望的方法，因此我们可以将这次采样化为下式：</p><p><img src="https://zlisnail.cn/img/image-20231017220710545.png" alt="image-20231017220710545"></p><p>这个形式就符合RM算法中的带有噪音的g函数的形式。受此启发，我们可以将求解$J(w)$最小值的SGD算法，看做是求解$\nabla{J(w)}=0$的RM算法（因为GD算法可以得到全局最优解，所以J是凸函数，所以可以这样转换）。如下图：</p><p><img src="https://zlisnail.cn/img/image-20231017221512368.png" alt="image-20231017221512368"></p><p><img src="https://zlisnail.cn/img/image-20231017221532513.png" alt="image-20231017221532513"></p><p>于是，我们可以直接套用RM的收敛条件，来说明SGD的收敛性：</p><p><img src="https://zlisnail.cn/img/image-20231017221700636.png" alt="image-20231017221700636"></p><p> 这里边只有第一个条件有些特殊，它表示函数$f$必须是凸函数。</p><h4 id="SGD实例"><a href="#SGD实例" class="headerlink" title="SGD实例"></a>SGD实例</h4><p>我们优化如下函数：</p><p><img src="https://zlisnail.cn/img/image-20231017215204127.png" alt="image-20231017215204127"></p><p>易证，这是一个凸函数，所以可以用GD获得全局最优解；且最优解w^*^=E(X)。GD与SGD算法书写如下：</p><p><img src="https://zlisnail.cn/img/image-20231017215452801.png" alt="image-20231017215452801"></p><p>可以观察到，这个问题的SGD算法与第一部分最后提出的mean estimation算法的形式一致。</p><p>假设w/X均为二维向量，有以下实例：</p><p><img src="https://zlisnail.cn/img/image-20231017223952295.png" alt="image-20231017223952295"></p><p>SGD收敛过程如下：</p><p><img src="https://zlisnail.cn/img/image-20231017224022509.png" alt="image-20231017224022509"></p><h4 id="SGD的性质"><a href="#SGD的性质" class="headerlink" title="SGD的性质"></a>SGD的性质</h4><p>从上图可以看到，当w离最优值w^<em>^比较远时，SGD的行为接近于GD，向最优值快速收敛；当w离最优值w^</em>^比较近时，SGD就与GD的行为差距越来越大，体现出随机性。 证明如下，利用了中值定理和函数的凸性（证明中假设w为标量，没有给出向量的证法）：</p><p><img src="https://zlisnail.cn/img/image-20231017222600470.png" alt="image-20231017222600470"></p><p><img src="https://zlisnail.cn/img/image-20231017222614931.png" alt="image-20231017222614931"></p><p><img src="https://zlisnail.cn/img/image-20231017222632629.png" alt="image-20231017222632629"></p><h4 id="无随机变量的SGD"><a href="#无随机变量的SGD" class="headerlink" title="无随机变量的SGD"></a>无随机变量的SGD</h4><p>考虑下面的优化问题：</p><p><img src="https://zlisnail.cn/img/image-20231017224603224.png" alt="image-20231017224603224"></p><p>这个优化问题中没有随机变量，它的GD算法如下：</p><p><img src="https://zlisnail.cn/img/image-20231017224742091.png" alt="image-20231017224742091"></p><p>然而依旧是老问题，如果x~i~太多，会导致GD算法太复杂。那么我们就可以用单个x~i~的函数梯度值来替代：</p><p><img src="https://zlisnail.cn/img/image-20231017224913354.png" alt="image-20231017224913354"></p><p>这个思路很像SGD。而实际上，它就是SGD。这里，每次的x~k~是在所有的x~i~中等概率随机选取的。但是目标函数中没有随机变量，这个算法为什么是对的，又为什么是SGD呢？</p><p>要解释这个问题，我们引入一个随机变量X：</p><p><img src="https://zlisnail.cn/img/image-20231017225148267.png" alt="image-20231017225148267"></p><p>于是，目标函数被转化为下面的形式：<br><img src="https://zlisnail.cn/img/image-20231017225217380.png" alt="image-20231017225217380"></p><p>所以这个算法就是一个可以得到正确结果的SGD算法。注意，每次的x~k~是在所有的x~i~中等概率随机选取的，因此它可能被选取多次，也可能一次也没有被选中。 </p><p><img src="https://zlisnail.cn/img/image-20231017225434461.png" alt="image-20231017225434461"></p><h4 id="有关MBGD，BGD，SGD的讨论"><a href="#有关MBGD，BGD，SGD的讨论" class="headerlink" title="有关MBGD，BGD，SGD的讨论"></a>有关MBGD，BGD，SGD的讨论</h4><p><img src="https://zlisnail.cn/img/image-20231017230249728.png" alt="image-20231017230249728"></p><p>注意，MBGD的采样是在已有所有采样当中随机选择部分采样处理，而BGD是直接处理所有采样。</p><p><img src="https://zlisnail.cn/img/image-20231017230745372.png" alt="image-20231017230745372"></p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> 强化学习数学原理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记（四）-蒙特卡洛方法</title>
      <link href="/2023/10/10/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/"/>
      <url>/2023/10/10/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="强化学习笔记（四）—蒙特卡洛方法"><a href="#强化学习笔记（四）—蒙特卡洛方法" class="headerlink" title="强化学习笔记（四）—蒙特卡洛方法"></a>强化学习笔记（四）—蒙特卡洛方法</h1><h2 id="预置知识"><a href="#预置知识" class="headerlink" title="预置知识"></a>预置知识</h2><p>模型：可以理解为强化学习中的环境。例如，$ p(r|s,a) $以及$p(s’|s,a)$ 都是取决于环境的概率。我们用概率来表示与环境交互后环境给出的结果，可以理解为我们对环境进行了建模。model-based强化学习，一般是说，我们首先对环境有一个定量的认识，可能会用数据估计出一个模型，然后基于对环境的建模进行训练；而model-free强化学习，可以认为是在对环境一无所知的情况下训练agent。</p><p>蒙特卡洛方法是一种model-free的方法。蒙特卡洛方法也被称为统计模拟方法，是指通过使用随机数来解决很多计算问题的方法。他的工作原理就是两件事：不断抽样、逐渐逼近。</p><p>在这里，我们需要温习一下概率论中的大数定理：</p><p><img src="https://zlisnail.cn/img/image-20230918203422649.png" alt="image-20230918203422649"></p><p>这个定理说明，当我们采集了足够多的样本时，样本的平均数可以代表随机变量的期望值。按照赵老师的话说，就是monte-carlo estimation可以用来求解mean estimation的问题。 </p><h2 id="强化学习中的Monte-Carlo算法"><a href="#强化学习中的Monte-Carlo算法" class="headerlink" title="强化学习中的Monte-Carlo算法"></a>强化学习中的Monte-Carlo算法</h2><h3 id="MC-Basic-algorithm"><a href="#MC-Basic-algorithm" class="headerlink" title="MC Basic algorithm"></a>MC Basic algorithm</h3><p>该算法的关键在于，将policy iteration algorithm转化为model-free的。</p><p>复习一下策略迭代算法。它分为两个迭代步骤：PE与PI。</p><p><img src="https://zlisnail.cn/img/image-20230918211358871.png" alt="image-20230918211358871"></p><p>我们考虑PI。在PI中，$q_{π_k}(s,a)$是依赖于模型的。但是，我们可以从它的定义出发，将它写成另一种不依赖于模型的形式：</p><p><img src="https://zlisnail.cn/img/image-20230918211946086.png" alt="image-20230918211946086"></p><p>正如上图所说，我们可以基于与环境的交互数据（experience），求解这个mean estimation问题。具体步骤如下：</p><p><img src="https://zlisnail.cn/img/image-20230918215952791.png" alt=""></p><p>于是，就可以将policy iteration algorithm进行改进。PI不需要改动，只需要PE步骤替换为上图的算法：</p><p><img src="https://zlisnail.cn/img/image-20230918220627254.png" alt="image-20230918220627254"></p><p>也就是说，评估策略的方法，从原先计算state-value，到现在计算每个步骤的q值。</p><p>伪代码如下：</p><p><img src="https://zlisnail.cn/img/image-20230918221443535.png" alt="image-20230918221443535"></p><p>这就是MC Basic算法，这是赵老师自己起的名字，所以在其他教材中可能看不到这个名词。这个算法因为太过低效，所以没有实践价值，但是有助于理解MC方法。由于policy iteration algorithm本身是收敛的，所以MC Basic也是收敛的。</p><p>这里每次计算q值的一次episode的长度，同样也是适当即可。太长的话对于改善策略没有实际的帮助，而太短则会导致策略得不到最优策略。</p><h3 id="MC-exploring-starts"><a href="#MC-exploring-starts" class="headerlink" title="MC exploring starts"></a>MC exploring starts</h3><p>由于MC basic方法效率太低，现在我们来探索一种方法，来更高效地利用数据、更新策略。</p><h4 id="如何改进MC-basic"><a href="#如何改进MC-basic" class="headerlink" title="如何改进MC basic"></a>如何改进MC basic</h4><p>在MC basic当中，每次对于一个（s,a）的episode可以用下面的trajectory来表示：<br><img src="https://zlisnail.cn/img/image-20231009153413242.png" alt="image-20231009153413242"></p><p>其中，红圈是一个state-action pair。我们定义在一个episode中，state-action pair 每出现一次，就是一次visit。</p><p>对于一个episode，可以改进它的利用方法。在MC basic方法中，我们仅仅利用了第一个state-action pair的q值（这种方法称作<strong>initial-visit method</strong>）。实际上，episode中还有很多state-action pair，以它们本身为界，之后的trajectory也可以看做是它们的一次q值采样。如下图所示：<br><img src="https://zlisnail.cn/img/image-20231009154324033.png" alt="image-20231009154324033"></p><p>这种方法叫做<strong>data-efficient method</strong>。这种方法又分为两种处理方法：<br>一种是<strong>first-visit method</strong>。在一个episode中，同一个pair可能多次出现。first-visit method只会利用第一次出现的q值做迭代，也即使用trajectory最长的一次。</p><p>另一种是<strong>every-visit method</strong>。多次出现的pair，出现的不同q值，都会被利用。</p><hr><p>上面，我们讨论的是如何更高效地利用episode得到的数据。下面，我们讨论如何更快速地更新策略。</p><p>在MC basic中，我们在PE步骤先收集一个pair的所有episode结果，然后对他们求平均来估计q值。这种方法必须等待多个episode的结果，才能最终得到这次迭代的q值估计。</p><p>现在，我们每得到一次episode的结果，我们就用它来估计q值，进而PI改进策略。improve the policy episode-by-episode。显然，精确度会下降，但它不影响最终可以收敛到一个策略当中。</p><blockquote><p>GPI（generalized policy iteration），指的是一种算法框架。它所代表的的思想是，在PE与PI之间不断地切换，且PE得到的估计的v值或者q值不需要特别地精确。在PE与PI交替进行足够多次之后，一定能够收敛到一个最优策略中去。许多强化学习算法最终都会落入这个框架当中。</p><p><img src="https://zlisnail.cn/img/image-20231009163105135.png" alt="image-20231009163105135"></p></blockquote><h4 id="具体算法"><a href="#具体算法" class="headerlink" title="具体算法"></a>具体算法</h4><p>​    伪代码如下：</p><p><img src="https://zlisnail.cn/img/image-20231009164257996.png" alt="image-20231009164257996"></p><p>算法中可以发现一些应用了上述改进思想的点：</p><ul><li>这里是每次做一次episode，之后就立刻进行PI，改进策略，策略更新更加高效。该算法之比于MC basic，其实就类似于policy iteration之比于value iteration。</li><li>算法应用的是first-visit method，也可以改成every-visit method。</li><li>倒序考虑每个pair的g值，算法更高效。</li><li>做一次episode的始态pair是有讲究的。一个pair，要么出现在episode中间（visit），要么是一个episode的始态（start）。为了算法最终能够对所有pair都进行估计（防止我们遗漏掉一些pair，而这个pair恰巧在最优策略当中），最终，我们的算法要确保每个pair都至少作为一次episode的始态。这就是<strong>exploring starts</strong> 名字的由来。也因此，这种算法很难应用于实践。</li></ul><h3 id="MC-without-exploring-starts-MC-Epsilon-Greedy"><a href="#MC-without-exploring-starts-MC-Epsilon-Greedy" class="headerlink" title="MC without exploring starts-MC Epsilon-Greedy"></a>MC without exploring starts-MC Epsilon-Greedy</h3><p>由于exploring starts这样的性质过于耗时，现在，我们要想办法把它去掉。去掉的方式，是引入<strong>soft policy</strong>。所谓的soft，指的是策略有可能会选择任何一个动作，对于任何一个动作的概率均不为零。</p><blockquote><p>policy 分为deterministic policy （<em>e.g.</em> greedy policy）和 stochastic policy（<em>e.g.</em> soft policy）。</p></blockquote><p>对于soft policy，一个足够长的episode可以遍历所有的pair，从而可以去掉exploring starts的条件。</p><hr><p>现在，我们将soft policy中的ε-greedy policy融合在MC exploring starts方法当中去。ε-greedy policy定义如下：</p><p> <img src="https://zlisnail.cn/img/image-20231009172217514.png" alt="image-20231009172217514"></p><p>公式看起来有些复杂，实际上，ε是<strong>探索（exploration）</strong>的概率，而1-ε是<strong>利用（exploitation）</strong>的概率。利用时，就采取最优动作；探索时，就随机采取动作。ε的值决定了探索与利用的比例。  </p><p>要将它融合进算法中，非常简单，只需要选出最大q值的动作放在exploitation公式里即可：</p><p><img src="https://zlisnail.cn/img/image-20231010170815448.png" alt="image-20231010170815448"></p><p>伪代码如下：</p><p><img src="https://zlisnail.cn/img/image-20231009230741781.png" alt="image-20231009230741781"></p><p>这里使用了every-visit，这是因为episode少了，但是变长了，如果只用第一个g值会造成浪费。</p><blockquote><p>伪代码中，虽然写明了是用的every-visit，但是实际算法实现的似乎还是first-visit？待确认</p></blockquote><h4 id="实际效果"><a href="#实际效果" class="headerlink" title="实际效果"></a>实际效果</h4><p>在经典grid-world环境中，每次生成1000,000步长的episode，有机会两次迭代就能获得最优ε-greedy策略。</p><p><img src="https://zlisnail.cn/img/image-20231009233428911.png" alt="image-20231009233428911"></p><p>ε-greedy方法去除了exploring starts后，牺牲了最优性。我们只能通过它找到最优的ε-greedy策略。如下面的例子：</p><p><img src="https://zlisnail.cn/img/image-20231009235956900.png" alt="image-20231009235956900"></p><p>上图中，ε=0和ε=0.1时，它们的最优策略是<strong>一致的(consistence)</strong>，即每个状态上的最优动作是相同的。但是，当eps=0.2或0.5时，一致性就丢失了。在强化学习训练成熟后，实际应用中肯定不能用ε-greedy最优策略，因此，保持ε-greedy策略与greedy策略的一致性是很重要的，这就要求ε值不能太大。</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> 强化学习数学原理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>猫变虎</title>
      <link href="/2023/10/02/%E7%8C%AB%E5%8F%98%E8%99%8E/"/>
      <url>/2023/10/02/%E7%8C%AB%E5%8F%98%E8%99%8E/</url>
      
        <content type="html"><![CDATA[<h1 id="猫变虎"><a href="#猫变虎" class="headerlink" title="猫变虎"></a>猫变虎</h1><h2 id="项目目标"><a href="#项目目标" class="headerlink" title="项目目标"></a>项目目标</h2><p>现有两张图片，一只小猫和一只老虎：</p><p><img src="https://zlisnail.cn/img/kitty.png" alt="kitty"></p><p><img src="https://zlisnail.cn/img/tiger.png" alt="tiger"></p><p>要求将上图变为下图，并展示中间的渐变过程。</p><h2 id="代码运行声明"><a href="#代码运行声明" class="headerlink" title="代码运行声明"></a>代码运行声明</h2><p>项目仓库：<a href="https://github.com/foresnailis/KittyToTiger">https://github.com/foresnailis/KittyToTiger</a></p><p>参考代码：<a href="https://www.csie.ntu.edu.tw/~b97074/vfx_html/hw1.html#C5">https://www.csie.ntu.edu.tw/~b97074/vfx_html/hw1.html#C5</a></p><p>python代码文件，需要opencv+numpy库。确保img文件夹下存有kitty.png/tiger.png两图片后，运行该代码，中间帧图片生成在result文件夹下。</p><p>本项目不支持自定义特征线、图片、中间帧数量，有兴趣可自行阅读修改代码。</p><h2 id="具体算法"><a href="#具体算法" class="headerlink" title="具体算法"></a>具体算法</h2><p>参考文献：<a href="http://www.cs.princeton.edu/courses/archive/fall00/cs426/papers/beier92.pdf">Feature-Based Image Metamorphosis , SIGGRAPH 1992</a></p><h3 id="特征线与像素点的关系"><a href="#特征线与像素点的关系" class="headerlink" title="特征线与像素点的关系"></a>特征线与像素点的关系</h3><p>首先，为两张图画上特征线，如图：</p><p><img src="https://zlisnail.cn/img/%E7%89%B9%E5%BE%81%E7%BA%BF.png" alt="特征线"></p><p>左右图相应位置上的特征线共同成对，共九对特征线。特征线的作用在于确定两张图的对应关系。当然，具体的操作需要落实到每个像素点上。那么，该如何确定两张图之间像素点的对应关系呢？</p><p>假设现在只设置一对特征线，要将线之间的关系延展到点之间的关系，需要用到uv两个变量。其中，v是点到特征线的距离，u是点到特征线的垂直落点在整条线段的位置，用比例表示。当两张图的两个点对于图中特征线有相同的uv值，则两个像素点是对应的。</p><p><img src="https://zlisnail.cn/img/single-line.jpg" alt="single-line"></p><p><img src="https://zlisnail.cn/img/u_v_math.jpg" alt="u_and_v"></p><p>而对于多对特征线，同一个点可能对应另一张图中的多个点，因此需要对这多个点加权，得到最终的对应点。权重公式如下：</p><p><img src="https://zlisnail.cn/img/multi-line.jpg" alt="multi-line"></p><p><img src="https://zlisnail.cn/img/weight.jpg" alt="weight"></p><p><img src="https://zlisnail.cn/img/image-20231002051739189.png" alt="image-20231002051739189"></p><h3 id="生成中间帧"><a href="#生成中间帧" class="headerlink" title="生成中间帧"></a>生成中间帧</h3><p>确定了两张图中像素与像素之间的对应关系之后，我们就可以生成中间帧了。对于中间帧，我们需要使用线性插值法，按照比例生成中间帧的特征线。换句话讲，比如我们要生成一个猫到虎的中间帧，这个中间帧上的特征线的两个顶点坐标、长度、角度，就由相应的猫图和虎图上对应的两条特征线上的顶点坐标、长度、角度相加除以二得到。这是生成一个中间帧的情况，程序中生成了5个中间帧，因此需要按照五分之一等分。</p><p>得到中间帧的特征线后，就可以找到中间帧上每个像素点分别在猫图和虎图上的对应像素点。两个像素点有两种色彩值（RGB+alpha），再利用线性插值原理混合它们，即可得到中间帧像素点的色彩值。</p><blockquote><p>找到两张图上的像素点后，可能会出现对应像素点的坐标为浮点数。按照道理来讲，数字图像在opencv中，每个像素点的坐标都是整数。如何确定浮点数坐标对应像素的色彩值呢？可以使用双线性插值法。如下图：</p><p><img src="https://zlisnail.cn/img/bilinear.jpg" alt="bilinear"></p></blockquote><h2 id="算法结果"><a href="#算法结果" class="headerlink" title="算法结果"></a>算法结果</h2><p><img src="https://zlisnail.cn/img/results_0.jpg" alt="results_0"><img src="https://zlisnail.cn/img/results_1.jpg" alt="results_1"><img src="https://zlisnail.cn/img/results_2.jpg" alt="results_2"><img src="https://zlisnail.cn/img/results_3.jpg" alt="results_2"><img src="https://zlisnail.cn/img/results_4.jpg" alt="results_4"></p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 算法实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAMES101-现代计算机图形学入门-笔记（三）</title>
      <link href="/2023/09/26/GAMES101-%E7%8E%B0%E4%BB%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%85%A5%E9%97%A8-%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/"/>
      <url>/2023/09/26/GAMES101-%E7%8E%B0%E4%BB%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%85%A5%E9%97%A8-%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="GAMES101-现代计算机图形学入门-笔记（三）"><a href="#GAMES101-现代计算机图形学入门-笔记（三）" class="headerlink" title="GAMES101-现代计算机图形学入门-笔记（三）"></a>GAMES101-现代计算机图形学入门-笔记（三）</h1><h2 id="深入了解变换"><a href="#深入了解变换" class="headerlink" title="深入了解变换"></a>深入了解变换</h2><h3 id="旋转变换的正交性质"><a href="#旋转变换的正交性质" class="headerlink" title="旋转变换的正交性质"></a>旋转变换的正交性质</h3><p><strong>如果一个矩阵的逆等于它的转置，则该矩阵称为正交矩阵</strong>。</p><p>考虑旋转变换矩阵。如果要旋转一个-θ角度（也即顺时针旋转θ角度），得到的矩阵正好是原矩阵的转置。同时，根据变换的意义，旋转-θ角度实际上也是旋转θ角度的矩阵的逆。因此，对于一个<strong>旋转变换矩阵，它是一个正交矩阵</strong>。</p><p><img src="https://zlisnail.cn/img/image-20230917212335228.png" alt="image-20230917212335228"></p><h3 id="三维变换"><a href="#三维变换" class="headerlink" title="三维变换"></a>三维变换</h3><h4 id="三维空间中的变换矩阵"><a href="#三维空间中的变换矩阵" class="headerlink" title="三维空间中的变换矩阵"></a>三维空间中的变换矩阵</h4><p>首先，类比二维空间，我们可以得到三维空间中的变换矩阵：</p><p><img src="https://zlisnail.cn/img/image-20230917212748038.png" alt="image-20230917212748038"></p><p>缩放与平移都不复杂，稍微难理解一点的是旋转矩阵。</p><p>绕轴旋转的变换矩阵如下：<br><img src="https://zlisnail.cn/img/image-20230917213242705.png" alt="image-20230917213242705"></p><p>注意，这里的所有变换矩阵，都有一个视角的问题。在二维空间，y轴正方向在x轴正方向右侧的情况下，逆时针旋转为正角度。那么扩展到三维空间，同样在xy平面上的旋转（也即绕z轴旋转），实际上我们应该从z轴正方向看向z轴负方向。对于其它平面上的旋转也是一样的。这实际上就是<strong>按照右手法则旋转</strong>。图形学中考虑旋转问题，基本都是默认右手法则。</p><h4 id="自由旋转"><a href="#自由旋转" class="headerlink" title="自由旋转"></a>自由旋转</h4><p>​    上面，我们学习了绕特定的轴的旋转变换该如何表示。那么在实际应用中，旋转的轴是自由的，并不局限在三个轴之内。如何去定义自由旋转呢？</p><p>​    直观地想象，我们可以用三个基础轴的旋转来表示自由旋转。如下图：</p><p><img src="https://zlisnail.cn/img/image-20230917214338829.png" alt="image-20230917214338829"></p><p>这种角称为欧拉角。</p><p>Rodrigue推导出了另一种，可以用一个<strong>单位</strong>方向向量表示旋转轴的方法：<br><img src="https://zlisnail.cn/img/image-20230917214510089.png" alt="image-20230917214510089"></p><p>在该式中，向量<strong>n</strong>表示一个从原点出发的<strong>单位</strong>向量，α表示绕该轴旋转的角度。用这个式子就可以推导出最终的变换矩阵写法。当然，如果轴并不经过原点，就需要先用平移变换使得向量经过原点。</p><p>有关该式子的推导，整体上的思路其实就是把旋转向量分解成与轴平行的向量以及与轴垂直的向量，然后平行向量不变，垂直向量旋转α角度。详细推导可见：<a href="https://www.cnblogs.com/wtyuan/p/12324495.html">罗德里格斯旋转公式（Rodrigues’ rotation formula）推导 </a>。很有意思，以后可以自己推一推。这个博客里的推导步骤非常详细，唯一没有提及的概念叫做<strong>投影矩阵</strong>。关于这个，可以在<a href="https://www.zhihu.com/question/40049682">一个向量乘它的转置，其几何意义是什么？</a>中找到。</p><p>另外，关于旋转，还有一种四元角的概念。它的目的是解决旋转角度变换不能直接通过旋转矩阵加减得到的问题。待了解。</p><h4 id="万向节锁"><a href="#万向节锁" class="headerlink" title="万向节锁"></a>万向节锁</h4><p>在大部分引擎当中，有一个有意思的现象叫做万向节锁。它的大概意思是说，物体的local坐标系中（local对应于global，世界坐标系），x,y,z三轴是嵌套的，例如在unity中，嵌套顺序就是y-&gt;x-&gt;z。当旋转物体的y轴时，物体的x、z轴会同步旋转；而当旋转物体的z轴时，y、x轴都不会发生任何的改变。这就导致，当你将物体绕着x轴旋转90度后，物体的z轴变了，但y轴没动，导致z和y轴重叠在了一起，在一条直线上。此时旋转物体的local坐标系中的y轴，会发现物体实际上在绕着物体local坐标系中的z轴旋转。图形化表示可见：<a href="https://www.bilibili.com/video/BV1o5411c7ex/?spm_id_from=333.337.search-card.all.click&amp;vd_source=dce5e3a4e8bded8fa669fa6355c5d0ed">万向锁详解</a> 。可以用陀螺仪来理解，详见：<a href="https://www.bilibili.com/video/BV1YJ41127qe/?spm_id_from=333.999.0.0&amp;vd_source=dce5e3a4e8bded8fa669fa6355c5d0ed">“欧拉角旋转”产生“万向锁”的来源，以及如何避免万向锁</a>。(PS:上述视频作者不是我，如有冒犯立马删除引用)</p><p>看到评论里有个解释很有意思，“之所以嵌套，而不是完全相对而言，主要是因为世界空间和物体本身的空间必须具备某种联系，在Unity中，这种联系是通过共用y轴实现的，这种共用仅仅体现在旋转上。当物体旋转时，对于y分量的旋转始终就是世界坐标系的y分量，而不会随着其他x、z分量的旋转而改变，这就是作者所讲的y排第一位的意义。如果这种旋转轴全部相对，或者就使用物体空间本身的轴，那么物体本身与世界空间就没有办法产生联系，我猜测，这是游戏引擎设计的时候所考虑的，无法避免的一种存在有缺陷的设计。 ”这里讲到的，世界空间与物体本身空间必须具备的某种联系，我不能完全理解。有待进一步探究。</p><p>2023/10/15补充：在B站看到一个视频阐述了万向节锁的本质，<a href="https://www.bilibili.com/video/BV1Nr4y1j7kn/?spm_id_from=333.337.search-card.all.click&amp;vd_source=dce5e3a4e8bded8fa669fa6355c5d0ed">无伤理解欧拉角中的“万向死锁”现象</a>。在这个视频中，提到旋转变换永远是将模型从原点按照一定的旋转顺序来变换，比如上文提到的yxz；而造成万向锁这样的现象，是在用户实际操作的情况下，旋转变换从模型初始位置开始按照一定顺序旋转与用户希望基于现在的本地坐标系进一步进行平滑旋转之间产生的矛盾。</p><p>欧拉角表示，会导致二义性，且不利于插值，这是我们引入四元数的本质原因。<br><img src="https://pic2.zhimg.com/80/v2-35df951674e6517119af35a8dee7d0fd_720w.webp" alt="欧拉角表示的二义性"><br>在这个图中可以看到，先绕x轴旋转α角度再绕z轴旋转β角度，与直接绕x轴α-β角度得到的结果相同。这就是二义性，同一种旋转可以由两种变换表示。同时这也说明了万向节锁，当绕x轴旋转α角度、绕y轴旋转90度后，此时调整x轴-β角度与调整z轴β角度，它们的结果是相同的。</p><h3 id="view-camera-transformation-视图变换"><a href="#view-camera-transformation-视图变换" class="headerlink" title="view/camera transformation 视图变换"></a>view/camera transformation 视图变换</h3><p>计算机中，要想将一个3D场景显示在屏幕上，需要设置两个位置：模型位置以及摄像机位置；最后，还需要通过投影变换，使得最终的屏幕上显示画面。下图用一个拍照的例子，生动形象地说明了这一过程：</p><p><img src="https://zlisnail.cn/img/image-20230919153600379.png" alt="image-20230919153600379"></p><p>上述三个变换，称为MVP变换。</p><p>如何定义一个相机的位置？用三个向量：</p><p><img src="https://zlisnail.cn/img/image-20230924220609432.png" alt="image-20230924220609432"></p><p>为了能让后续的一些操作更加简便，我们要将相机变换到原点，朝向z轴负方向（右手坐标系中），up向量指向y轴正方向。如图所示：</p><p><img src="https://zlisnail.cn/img/image-20230924221901385.png" alt="image-20230924221901385"></p><p>变换矩阵如下：</p><p><img src="https://zlisnail.cn/img/image-20230924222834178.png" alt="image-20230924222834178"></p><p>上面的M~view~也可以理解为基变换的过渡矩阵，可以将xyz坐标系中的物体变换到相机坐标系中去。上述公式中，g,t,(g×t)都是单位向量。</p><h3 id="Projection-transformation-投影变换"><a href="#Projection-transformation-投影变换" class="headerlink" title="Projection transformation 投影变换"></a>Projection transformation 投影变换</h3><p>分为正交投影（Orthographic projection）和透视投影（Perspective projection。</p><p><img src="https://zlisnail.cn/img/image-20230924232249147.png" alt="image-20230924232249147"></p><p>一个直观的区别就是，透视投影使得原先平行的两条线不再平行。下面的图例可以更好地展示它们的本质区别：</p><p><img src="https://zlisnail.cn/img/image-20230924232653880.png" alt="image-20230924232653880"></p><h4 id="正交投影"><a href="#正交投影" class="headerlink" title="正交投影"></a>正交投影</h4><p>一个简单的方法：在上述视图变换的基础上，扔掉z轴，并将物体在xy平面上的投影平移缩放到[-1,1]^2^上去。</p><p><img src="https://zlisnail.cn/img/image-20230924234039922.png" alt="image-20230924234039922"></p><p>标准做法：</p><p>首先将物体平移到原点上去，然后将它缩放到[-1,1]^3^（canonical cube）。<strong><em>为什么？</em></strong></p><p><img src="https://zlisnail.cn/img/image-20230924234222216.png" alt="image-20230924234222216"></p><p>从上面的图例中可以看出，所谓的far面的z值比near面的z值小，有点不太符合直觉。所以openGL使用左手系，从而保持far面z值更大。</p><h4 id="透视投影"><a href="#透视投影" class="headerlink" title="透视投影"></a>透视投影</h4><p>按照闫老师的方法，透视投影本质上是远面向近面上的投影，需要先将远面压缩到与近面相同的大小，然后应用正交投影。</p><p><img src="https://zlisnail.cn/img/image-20230925001515494.png" alt="image-20230925001515494"></p><p>没有完全理解，先把公式贴在这里：</p><p><img src="https://zlisnail.cn/img/image-20230925002544038.png" alt="image-20230925002544038"></p><p><img src="https://zlisnail.cn/img/image-20230925002604043.png" alt="image-20230925002604043"></p><p><img src="https://zlisnail.cn/img/image-20230925002615777.png" alt="image-20230925002615777"></p><p><img src="https://zlisnail.cn/img/image-20230925002645863.png" alt="image-20230925002645863"></p>]]></content>
      
      
      <categories>
          
          <category> 计算机图形学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机图形学 </tag>
            
            <tag> GAMES101 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAMES101-现代计算机图形学入门-笔记（二）</title>
      <link href="/2023/09/16/GAMES101-%E7%8E%B0%E4%BB%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%85%A5%E9%97%A8-%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2023/09/16/GAMES101-%E7%8E%B0%E4%BB%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%85%A5%E9%97%A8-%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="GAMES101-现代计算机图形学入门-笔记（二）"><a href="#GAMES101-现代计算机图形学入门-笔记（二）" class="headerlink" title="GAMES101-现代计算机图形学入门-笔记（二）"></a>GAMES101-现代计算机图形学入门-笔记（二）</h1><h2 id="变换transformation"><a href="#变换transformation" class="headerlink" title="变换transformation"></a>变换transformation</h2><h3 id="二维变换"><a href="#二维变换" class="headerlink" title="二维变换"></a>二维变换</h3><h4 id="Scale-缩放"><a href="#Scale-缩放" class="headerlink" title="Scale 缩放"></a>Scale 缩放</h4><p>​    均匀缩放变换：</p><p><img src="https://zlisnail.cn/img/image-20230916154811977.png" alt="缩放的矩阵变换"></p><p>不均匀缩放变换：</p><p><img src="https://zlisnail.cn/img/image-20230916154850850.png" alt="不均匀缩放变换"></p><p>对称变换：</p><p><img src="https://zlisnail.cn/img/image-20230916155032633.png" alt="image-20230916155032633"></p><p>​    切变shear matrix：</p><p>​    <img src="https://zlisnail.cn/img/image-20230916155338091.png" alt="切变变换"></p><h4 id="Rotate-旋转"><a href="#Rotate-旋转" class="headerlink" title="Rotate 旋转"></a>Rotate 旋转</h4><p>​     <img src="https://zlisnail.cn/img/image-20230916160705027.png" alt="image-20230916160705027"></p><p>上述变换均为线性变换</p><h4 id="Translation-平移"><a href="#Translation-平移" class="headerlink" title="Translation 平移"></a>Translation 平移</h4><p><img src="https://zlisnail.cn/img/image-20230916161449984.png" alt="image-20230916161449984"></p><p><img src="https://zlisnail.cn/img/image-20230916161641129.png" alt="image-20230916161641129"></p><blockquote><p>注意，在上图中，（x,y）是先经过线性变换，再平移。注意它的几何意义。</p></blockquote><p>平移变换不是线性变换，而是仿射变换。</p><p>附：线性变换的定义：<br><img src="https://zlisnail.cn/img/image-20230916163032476.png" alt="image-20230916163032476"></p><p>于是，为了将平移变换与其它变换的处理方式统一，人们增加了一个维度，引入了齐次坐标：</p><p><img src="https://zlisnail.cn/img/image-20230916163537144.png" alt="image-20230916163537144"></p><p>点与向量增加一个维度后，表示如下：</p><p><img src="https://zlisnail.cn/img/image-20230916163917737.png" alt="image-20230916163917737"></p><p>注意，在上面的第三个维度中，点与向量的表示方法不同。点的w值设置为1，向量的w值设置为0。这种表示方式，可以从很多个角度来理解。</p><p>比如，在平移变换中，点的平移确实会改变它的位置，即xy值；但向量平移，表示方式不能变，即xy不能变。如此，将w值设置为0，可以让向量在变换时屏蔽掉矩阵中的第三列，使得最终变换后的向量的坐标表示不受平移的影响。</p><p>从另一个角度，这种表示方式可以最大程度上满足点与向量加减法的数值解与实际意义之间的对应关系。考虑下图：</p><p><img src="https://zlisnail.cn/img/image-20230916165106083.png" alt="image-20230916165106083"></p><p>在这种表示方法下，向量加向量依旧能够得到一个向量；点减去点正好就是一个向量；点加上向量恰好可以得到一个点。需要特殊记忆的是点加点的算法。在齐次坐标表示法中，定义一个点的w值必须是1。也就是说，两个点的坐标直接相加后，因为w值为2，因此必须将xyw值同除以2，才能使得此时的结果xy正常表示点的坐标。换句话讲，齐次坐标表示下的点点相加，得到的点是两点连成线段的中点。</p><h4 id="齐次坐标-homogenous-coordinates"><a href="#齐次坐标-homogenous-coordinates" class="headerlink" title="齐次坐标 homogenous coordinates"></a>齐次坐标 homogenous coordinates</h4><p>​    上述所有变换可以统一到齐次坐标下。</p><p><img src="https://zlisnail.cn/img/image-20230916165735586.png" alt="image-20230916165735586"></p><p><img src="https://zlisnail.cn/img/image-20230916165845055.png" alt="image-20230916165845055"></p><p>注意，这里是在二维坐标下的仿射变换。它们的最后一行都是0，0，1。</p><h4 id="逆变换-inverse-transform"><a href="#逆变换-inverse-transform" class="headerlink" title="逆变换 inverse transform"></a>逆变换 inverse transform</h4><p>​    在数学上，逆变换就是原先仿射矩阵的逆矩阵。</p><p><img src="https://zlisnail.cn/img/image-20230916170241082.png" alt="image-20230916170241082"></p><h4 id="变换组合"><a href="#变换组合" class="headerlink" title="变换组合"></a>变换组合</h4><p>​    变换顺序是不能随便调换的；在数学意义上，矩阵乘法是没有交换律的。</p><p><img src="https://zlisnail.cn/img/image-20230916170559529.png" alt="image-20230916170559529"></p><p>如上图，注意变换是从右到左应用的。</p><p>也可以应用矩阵的乘法结合律，将所有简单的矩阵变换乘在一起，得到一个复杂的3×3矩阵，它表示将所有简单变换融合后的最终变换。</p><h4 id="绕特定点旋转的表示方法"><a href="#绕特定点旋转的表示方法" class="headerlink" title="绕特定点旋转的表示方法"></a>绕特定点旋转的表示方法</h4><p>​    由于旋转变换只能绕着原点，如果要绕着别的点旋转，必须先平移，将该点平移到原点的位置，再去旋转。</p><p>​    <img src="https://zlisnail.cn/img/image-20230916171454126.png" alt="image-20230916171454126"></p><h3 id="三维变换"><a href="#三维变换" class="headerlink" title="三维变换"></a>三维变换</h3><p>三维变换也需要平移，因此也需要使用齐次坐标。</p><p><img src="https://zlisnail.cn/img/image-20230916171830697.png" alt="image-20230916171830697"></p><p><img src="https://zlisnail.cn/img/image-20230916171924860.png" alt="image-20230916171924860"></p>]]></content>
      
      
      <categories>
          
          <category> 计算机图形学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机图形学 </tag>
            
            <tag> GAMES101 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAMES101-现代计算机图形学入门-笔记（一）</title>
      <link href="/2023/09/16/GAMES101-%E7%8E%B0%E4%BB%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%85%A5%E9%97%A8-%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2023/09/16/GAMES101-%E7%8E%B0%E4%BB%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%85%A5%E9%97%A8-%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="GAMES101-现代计算机图形学入门-笔记（一）"><a href="#GAMES101-现代计算机图形学入门-笔记（一）" class="headerlink" title="GAMES101-现代计算机图形学入门-笔记（一）"></a>GAMES101-现代计算机图形学入门-笔记（一）</h1><h2 id="一些基础概念"><a href="#一些基础概念" class="headerlink" title="一些基础概念"></a>一些基础概念</h2><p>实时：&gt;=30FPS      离线</p><p>右手坐标系：x轴方向向量叉乘y轴方向向量，结果向量与z轴方向一致</p><p>有些图形学API中，例如openGL，使用的是左手坐标系</p><p>向量叉乘满足乘法分配律</p><p><img src="https://zlisnail.cn/img/image-20230916140616597.png" alt="image-20230916140616597"></p><p><img src="https://zlisnail.cn/img/image-20230916140837379.png" alt="image-20230916140837379"></p><p>向量点乘可以判断两个向量在方向上的一致性。当两个向量的点乘结果大于0时，向量在方向上一定程度上同向；等于0时，向量垂直；小于0时，向量异向。</p><p>向量叉乘可以判断向量的左右关系。对于一个右手坐标系，有a、b两向量在xy平面上。如果a×b的结果向量指向z正方向，则说明b在a的左侧；如果指向负方向，则说明b在a的右侧。这里的左右，直观上判断，实际上就是以a的正方向与a的负方向为两个射线，将整个平面划分为两大块后，视角对准a的正方向，左与右也就随之确定。</p><p><img src="https://zlisnail.cn/img/image-20230916142039535.png" alt="ima-20230916142039535"></p><p>这种用向量叉乘判断左右的方法，可以用来判断一个像素点是否在一个三角形内部。上图右侧中，向量AB×向量AP，指向z轴正方向；同样的，向量BC×向量BP，以及向量CA×向量CP，均指向z轴正方向。这也就说明了三个以P为终点的向量，均在三角形对应三条边的左侧，也就意味着P点在三角形的内部。当然，这是在ABC三个顶点逆时针排列的情况下。当三个顶点顺时针排列时，同样的三次叉乘，它们的结果都应该指向z轴的反方向，才能表示P点在三角形内部。不管三个顶点怎样排列，判断一个点是否在一个三角形面中的方法很简单，只需要判断三次叉乘的结果，是否均指向同一个方向。</p><p>三维空间中，任意向量在任意三维直角坐标系中的投影计算方法： 与三轴的单位正向量点乘，得到投影。如下图：</p><p><img src="https://zlisnail.cn/img/image-20230916142945775.png" alt="image-20230916142945775"></p>]]></content>
      
      
      <categories>
          
          <category> 计算机图形学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机图形学 </tag>
            
            <tag> GAMES101 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记（三）--值迭代与策略迭代</title>
      <link href="/2023/09/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89-%E5%80%BC%E8%BF%AD%E4%BB%A3%E4%B8%8E%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3/"/>
      <url>/2023/09/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89-%E5%80%BC%E8%BF%AD%E4%BB%A3%E4%B8%8E%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="强化学习笔记（三）—值迭代与策略迭代"><a href="#强化学习笔记（三）—值迭代与策略迭代" class="headerlink" title="强化学习笔记（三）—值迭代与策略迭代"></a>强化学习笔记（三）—值迭代与策略迭代</h1><p>首先，再次复习一下贝尔曼公式的matrix-vector数学形式：</p><script type="math/tex; mode=display">v=f(v)=\mathop{max}\limits_{π}(r_π+γP_πv)</script><h2 id="value-iteration-algorithm"><a href="#value-iteration-algorithm" class="headerlink" title="value iteration algorithm"></a>value iteration algorithm</h2><p>值迭代方法，实际上就是在强化学习笔记（二）中提到的迭代算法。数学表达式如下：</p><script type="math/tex; mode=display">v_{k+1}=f(v_k)=\mathop{max}\limits_{π}(r_π+γP_πv_k),\ \ \ \ \ \ \ \ \  k=1,2,3...</script><p>在（二）中，我们已经证明了，利用这样的迭代算法，最终可以得到一个不动点<code>v*</code>，作为最优的state-value。这个算法可以分为两个步骤：</p><h4 id="policy-update"><a href="#policy-update" class="headerlink" title="policy update"></a>policy update</h4><p>​    <img src="https://zlisnail.cn/img/image-20230915205055682.png" alt="image-20230915205055682"></p><p>当然，（二）中，我们已经推导出，π就是贪婪策略</p><h4 id="value-update"><a href="#value-update" class="headerlink" title="value update"></a>value update</h4><p>​    <img src="https://zlisnail.cn/img/image-20230915205125411.png" alt="image-20230915205125411"></p><p>注意，在第二步value update中展示的式子，并不是贝尔曼公式。贝尔曼公式中，等号左右两式的v是相同的v，是state-value；而这里，左右两式中的v仅仅是值，它们不相等，且没有任何特殊含义，更不是state-value。</p><p>上述步骤伪代码如下：</p><p><img src="https://zlisnail.cn/img/image-20230915212311986.png" alt="值迭代伪代码"></p><h2 id="policy-iteration-algorithm"><a href="#policy-iteration-algorithm" class="headerlink" title="policy iteration algorithm"></a>policy iteration algorithm</h2><p>上面的值迭代算法中，每次迭代都会得到一个新的v~k~值。而在策略迭代算法中，每次迭代都会得到一个新的策略π~k+1~，这个策略是基于由π~k~生成的贝尔曼公式的解v~k~得来的，依旧应用贪婪的思想。具体数学表达式如下：</p><h4 id="policy-evaluation"><a href="#policy-evaluation" class="headerlink" title="policy evaluation"></a>policy evaluation</h4><p>​    <img src="https://zlisnail.cn/img/image-20230916230045158.png" alt="image-20230916230045158"></p><p>这一步实际上是基于π~k~求解贝尔曼公式。如学习笔记（一）中所说，简单的公式可以直接使用线性方程组解出；复杂的可以使用迭代算法。</p><h4 id="policy-improvement"><a href="#policy-improvement" class="headerlink" title="policy improvement"></a>policy improvement</h4><p>​    <img src="https://zlisnail.cn/img/image-20230916230119101.png" alt="image-20230916230119101"></p><p>这里的π~k+1~一定是优于π~k~的，这个证明在赵老师的书中可以找到。（这次就先略过）</p><p>当然，依旧是贪婪策略。</p><p>整体过程如下：</p><p><img src="https://zlisnail.cn/img/image-20230916230204055.png" alt="image-20230916230204055"></p><p>整个迭代过程，最终一定能够收敛到最优策略、最优state-value。证明同样在赵老师的书中。（略过）</p><p>伪代码如下：</p><p><img src="https://zlisnail.cn/img/image-20230916230857999.png" alt="image-20230916230857999"></p><h4 id="truncated-policy-iteration"><a href="#truncated-policy-iteration" class="headerlink" title="truncated policy iteration"></a>truncated policy iteration</h4><p>仔细比对上面的两个算法，发现其实它们在很大部分上相同：</p><p><img src="https://zlisnail.cn/img/image-20230916231615533.png" alt="image-20230916231615533"></p><p> 不同的是，在求解下一个迭代的v值时，值迭代算法仅计算了一次（没有迭代），而策略迭代算法则不断迭代v直至收敛。如下图：</p><p><img src="https://zlisnail.cn/img/image-20230916231825941.png" alt="image-20230916231825941"></p><p>截断策略迭代算法，既避免策略迭代算法中求解贝尔曼公式时迭代次数过多的问题，也防止因为值迭代算法在每次迭代中计算v值时过于简单导致整体迭代次数过多的问题。</p><p>因此，截断策略迭代算法的代码几乎与策略迭代算法相同，唯一不同的在于求解贝尔曼公式时设置了一个最大迭代次数。（原先是判断误差是否小于某数）</p><p><img src="https://zlisnail.cn/img/image-20230916232124939.png" alt="image-20230916232124939"></p><p>这个算法一定是可以收敛的，证明同样在赵老师的书中。以后再看吧（汗，多少证明没看-_-||）。</p><p>直观的三种算法的比较如下图，其中横坐标是迭代次数。</p><p><img src="https://zlisnail.cn/img/image-20230916232419214.png" alt="image-20230916232419214"></p><p>伪代码中j~truncated~的值对收敛速度的影响：</p><p><img src="https://zlisnail.cn/img/image-20230916232522198.png" alt="image-20230916232522198"></p><p>可以看到，设置适当的j~truncated~值是必要的。</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> 强化学习数学原理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记（二）--贝尔曼最优公式</title>
      <link href="/2023/09/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89-%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F/"/>
      <url>/2023/09/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89-%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E5%85%AC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="强化学习笔记（二）—贝尔曼最优公式"><a href="#强化学习笔记（二）—贝尔曼最优公式" class="headerlink" title="强化学习笔记（二）—贝尔曼最优公式"></a>强化学习笔记（二）—贝尔曼最优公式</h1><p>贝尔曼最优公式（Bellman optimality equation,BOE）实际上就是利用贝尔曼公式，求解使得所有state的v值最大的策略π，以及相应的v值。数学上，只需要在贝尔曼公式的右式添加一个max~π~即可。也就是说，贝尔曼公式，对应一个策略π，它的策略是给定的，非变量；而贝尔曼最优公式，则是将π值作为变量，需要找到最优的π*。</p><p>从另一个角度理解，也可以讲，贝尔曼最优公式是一种特殊的贝尔曼公式，它的策略就是π<em>，得到的v值就是最优值v\</em>。</p><p>求解BOE是一个最优化问题。可以按照下面的思路进行求解π值：</p><p><img src="https://zlisnail.cn/img/image-20230914212226037.png" alt="贝尔曼最优公式"></p><p>于是，最优策略实际上就是使得每个s处都取q值最大action的策略π。（<strong>贪婪</strong>策略）</p><p><img src="https://zlisnail.cn/img/image-20230914212405923.png" alt="贪婪策略"></p><p>有关这个策略是否能得到最优值，还有一个定理可以证明。定理如下，证明在赵老师的书里。（话说为什么还要再证明一次？）</p><p><img src="https://zlisnail.cn/img/image-20230914232905082.png" alt="最优策略定理"></p><p>上面的结果，给出了求最优值时的具体策略，即π值。于是，π的值确定。那么我们要如何求解v值呢？变换贝尔曼最优公式，我们可以得到如下式子：</p><p><img src="https://zlisnail.cn/img/image-20230914212838417.png" alt="化简贝尔曼最优公式到v=f(v)的形式"></p><h4 id="contraction-mapping-收缩映射"><a href="#contraction-mapping-收缩映射" class="headerlink" title="contraction mapping 收缩映射"></a>contraction mapping 收缩映射</h4><p><img src="https://zlisnail.cn/img/image-20230914222955819.png" alt="contraction mapping"></p><h4 id="fixed-point-不动点"><a href="#fixed-point-不动点" class="headerlink" title="fixed point 不动点"></a>fixed point 不动点</h4><p>​    <img src="https://zlisnail.cn/img/image-20230914223020975.png" alt="fixed point"></p><h4 id="contraction-mapping-theorem-收缩映射定理"><a href="#contraction-mapping-theorem-收缩映射定理" class="headerlink" title="contraction mapping theorem 收缩映射定理"></a>contraction mapping theorem 收缩映射定理</h4><p>​    该定理未给出证明，之后可以研究下：</p><p><img src="https://zlisnail.cn/img/image-20230914223309271.png" alt="contraction mapping theorem"></p><p>同时，我们也可以证明，贝尔曼最优公式就是一个收缩映射（证明待查）：<br><img src="https://zlisnail.cn/img/image-20230914223743541.png" alt="贝尔曼最优公式是一个收缩映射"></p><p>于是，应用收缩映射定理，我们可以轻易得到，贝尔曼最优公式的迭代算法：</p><p><img src="https://zlisnail.cn/img/image-20230914224340807.png" alt="贝尔曼最优公式应用收缩映射定理"></p><p>换句话讲，现在我们清楚地知道，贝尔曼最优方程有且仅有唯一的不动点v，且可以通过不断迭代得到。很明显，这个不动点，就是BOE的解。（注意，最优的v是唯一的，但是最优的策略π不一定）</p><h4 id="value-iteration-algorithm"><a href="#value-iteration-algorithm" class="headerlink" title="value iteration algorithm"></a>value iteration algorithm</h4><p><img src="https://zlisnail.cn/img/image-20230914225730112.png" alt="程序算法"></p><h4 id="奖励对策略的影响"><a href="#奖励对策略的影响" class="headerlink" title="奖励对策略的影响"></a>奖励对策略的影响</h4><p>对r作仿射变换，即r-&gt;ar+b，最终策略不会有什么变化。不同状态r值之间的相对关系才是决定策略的关键因素。</p><p> <img src="https://zlisnail.cn/img/image-20230915171837719.png" alt="奖励的线性变换对v的影响"></p><p>由上图可以看到，BOE中的因子式</p><script type="math/tex; mode=display">γP_πv</script><p>也会被仿射变换，而贪心策略一直都在选择最大q值的动作，因此最终策略不会有变化。</p><p>（这里没有说明a能否小于0。如果a小于0，原本q值最大的动作，反而成为最小的，这不就使得策略改变吗？待讨论）</p><p>最后，再对γ进行一些补充讨论：</p><p><img src="https://zlisnail.cn/img/image-20230915174613099.png" alt="一个例子"></p><p>在上图中，左边是最优策略，右边是非最优的。前面我们知道，所谓最优，指的是每个v值都是最大的，换句话说，从每个状态s出发的轨迹，得到的累积奖励return都是最大的。为什么（1,2）处的state-value不是最优的v值呢？因为它在此处的策略是向左走的。向左走，导致它的return式子为下图b式：</p><p><img src="https://zlisnail.cn/img/image-20230915175032973.png" alt="两种策略的return值"></p><p>显然，由于γ的存在，路径越长的轨迹，越不容易获得高的return。于是，继续迭代BOE，我们就会得到结果a。这也给我们设计奖励带来启示，不需要可以设置惩罚（即负数的奖励），有时候直接设计为0即可。</p><p>​    </p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> 强化学习数学原理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习笔记（一）--贝尔曼公式</title>
      <link href="/2023/09/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%85%AC%E5%BC%8F/"/>
      <url>/2023/09/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%85%AC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h4 id="贝尔曼公式学习"><a href="#贝尔曼公式学习" class="headerlink" title="贝尔曼公式学习"></a>贝尔曼公式学习</h4><p>​    trajectory（轨迹）的return（返回值），用于衡量某策略的优劣，即一个状态转移链条的最终reward值。</p><p>​    计算过程中，对于长度较长的trajectory，一般有γ作为折扣因子。折扣因子较小，表示策略重视近期效果；折扣因子较大，表示策略重视远期效果。</p><p>​    观察下面的例子：</p><p><img src="https://zlisnail.cn/img/image-20230907161312156.png" alt="image-20230907161312156"></p><p>这里，value~i~的定义是从s~i~出发的无限长度轨迹的return值。</p><p>上述情况，表明一个状态的value，依赖于其他状态的value。</p><p>于是，利用线性代数的知识，就可以解出该方程组，获得所有的value值：</p><p><img src="https://zlisnail.cn/img/image-20230907164151117.png" alt="image-20230907164151117"></p><p> 上面是对核心概念的理解，做了简化。下图则定义了强化学习最常用的几个变量。注意，大写的S/A/R，它们都是<strong>随机变量（random variables）</strong>。随机变量可以通过概率公式求出，可以对他们求期望等。</p><p><img src="https://zlisnail.cn/img/image-20230907170454906.png" alt="image-20230907170454906"></p><p>​      由此，将多步连接起来，就得到了一个trajectory。在这个轨迹中，使用<em>G~t~</em>来表示discounted return。 于是，我们可以定义一个重要的变量：state-value function（简称state-value）。</p><p><img src="https://zlisnail.cn/img/image-20230907171058820.png" alt="image-20230907171058820"></p><p>由定义公式可见，该函数的自变量包括当前状态s和策略π。</p><p>区别return与state value，return 是单个轨迹的总奖励值；而state value是从当前状态出发按照一定的策略，可能得到的所有轨迹的奖励值的<strong>期望</strong>。</p><p>由上述定义公式，结合trajectory的概念，可以轻易地得出下式：</p><p> <img src="https://zlisnail.cn/img/image-20230911212029725.png" alt="image-20230911212029725"></p><p>其中，第一项的计算方法：</p><p><img src="https://zlisnail.cn/img/image-20230912145856573.png" alt="image-20230912145856573"></p><p>第二项的计算方法：</p><p><img src="https://zlisnail.cn/img/image-20230912150431461.png" alt="image-20230912150431461"></p><p>由此得到贝尔曼公式：</p><p><img src="https://zlisnail.cn/img/image-20230912150850381.png" alt="image-20230912150850381"></p><p>所有s都有一个这样的式子，将所有式子联立，就可以得到一个方程组，解出所有的v。这个式子中，π就是策略；两个p就是环境模型。解出这个式子，得到v值，实际上也就是在<strong>评估一个策略</strong>（policy evaluation）。（从v的下标为π中，我们也可以看出，π是这个式子的关键）。</p><p>分析这个式子，可以看到，state-value首先根据不同的动作去分类，然后分别考虑每个动作获得的即时奖励和长远奖励（用下一个状态的state-value来表示，state-value可以理解为一个状态的价值；价值越高，未来越有可能获得高奖励 ）。</p><p>上述式子便于理解，但是是独立的。下面，我们来推导出贝尔曼公式的矩阵形式。</p><p>首先，对上式进行简化，提炼出该式子的核心：即时奖励与长远奖励。</p><p><img src="https://zlisnail.cn/img/image-20230912154047641.png" alt="image-20230912154047641"></p><p>将上式中的v\r\P，替换成向量或矩阵，就可以得到：</p><p><img src="https://zlisnail.cn/img/image-20230912154850369.png" alt="image-20230912154850369"></p><p>注意，上图里，大写字母P代表状态转移矩阵。矩阵有一种下标表示的写法，值得学习。另外，在定义向量的时候，规定向量每个元素的范围的写法，也非常有意思；R^n^既可以表示向量的长度，也可以表示向量元素的取值范围。别忘了将向量转置。</p><p>例子：</p><p><img src="https://zlisnail.cn/img/image-20230912165518172.png" alt="image-20230912165518172"></p><p>由上面的贝尔曼方程，我们可以轻易得到一种v的解法，那就是：</p><p><img src="https://zlisnail.cn/img/image-20230912175304959.png" alt="image-20230912175304959"></p><p>但是该解法需要对矩阵求逆（该矩阵一定是有逆的，因为当γ小于1时，该矩阵为严格对角占优矩阵），当状态空间非常大时，求逆运算会非常复杂。于是，有以下迭代算法：</p><p><img src="https://zlisnail.cn/img/image-20230912175418012.png" alt="image-20230912175418012"></p><p>其中，v~0~即为r~π~。当k趋于无穷时，v~k~无穷接近于v~π~，证明如下：</p><p><img src="https://zlisnail.cn/img/image-20230912175637624.png" alt="image-20230912175637624"></p><h5 id="action-value"><a href="#action-value" class="headerlink" title="action value"></a>action value</h5><p>​    直译作动作价值，实际上当前状态也是因子：</p><p><img src="https://zlisnail.cn/img/image-20230912224228146.png" alt="image-20230912224228146"></p><p>​    定义式子如下，用q~π~来表示：</p><p><img src="https://zlisnail.cn/img/image-20230912224645939.png" alt="image-20230912224645939"></p><p>从这个图中就可以看到，实际上q~π~（s,a）就是将v~π~(s)中的因子替换掉（即时奖励+长久奖励）：</p><p><img src="https://zlisnail.cn/img/image-20230912224916522.png" alt="image-20230912224916522"></p><p>至此，本节结束。贝尔曼公式是整个强化学习的基础，后续会多次用到。若未来对贝尔曼公式有更深的理解，该笔记将继续更新。</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> 强化学习数学原理 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
